\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{url}

\title{Global curvature of data}
\author{}
\date{}

\usepackage{natbib}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{wasysym}
\usepackage{amsthm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                Setup of the theorem environment                                 %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{assumption}[definition]{Assumption}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{remark}[theorem]{Remark}

\begin{document}

\maketitle

\section{Status and ToDo}

ToDo:
\begin{itemize}
    \item Write down some simple heuristics (Liuda)
    \item Estimate distortion for geometric graphs (Pim)
    \item Understand the details and implications for Section 3.4 in the paper on product spaces, because there is a theoretical result on distortion of general graphs (Liuda)
\end{itemize}

Ideas for research:
\begin{itemize}
    \item Gromov's hyperbolicity cannot be easily converted to curvature. We can think about some similarly motivated measure which can be. Using either the distance from one edge of a triangle to the other edges or using the perimeter of the orthic triangle. 
    \item Think about optimal curvature of ER graphs. We have the result on  Ollivier curvature, but it is supposed to be different (for example, Ollivier is bounded between -2 and 1)
    \item We all understand now that distortion is not a good measure for graph embeddings from all points of view. One reason is that optimal distortion is $-\infty$ for many graphs (trees and tree-like). Let's compare this with some threshold-based loss function.
\end{itemize}

\section{Selected relevant papers}

\paragraph{Learning Mixed-Curvature Representations in Product Spaces~\cite{gu2019learning}}

Embed graphs to a product manifold combining multiple copies of spherical,
hyperbolic, and Euclidean spaces. The code is available at \url{https://github.com/HazyResearch/hyperbolics}. 
This is the main paper for the current project: we use their code for embedding, we also use two their approaches as our baselines: gradient descent and parallelogram. 

\paragraph{Representation Tradeoffs for Hyperbolic Embeddings~\cite{sala2018representation}}

Interesting observation that not only dimensionality, but also precision is important.
Propose a new algorithm for graph embedding: first, embed a graph into a weighted tree, and then embed that tree into the hyperbolic disk. The code is available at \url{https://github.com/HazyResearch/hyperbolics}.
One of their observations, which is quite natural, is that the dependence of the required precision on $deg_{max}$ is logarithmic, but on the maximum path length is linear.

When they embed a tree to a hyperbolic disk (ball) they use Sarkar's algorithm for a 2-dimensional disk~\cite{sarkar2011low}. This algorithm is simple: take the next node, consider its children, distribute them uniformly on a circle around the node, as far as possible from the parent of this node. The authors also extend Sarkar's algorithm to higher dimensions, where they use spherical coding to space out points on a hypersphere. 

An important ingredient of Sarkar's algorithm is scaling $\tau$: we assume that the length of an edge is not 1, but $\tau$. Larger $\tau$ allows to reduce distortion (increasing $\tau$ basically means increasing absolute value of curvature), but larger $\tau$ requires larger precision also, so there is a trade-off. 

A theoretical result about embedding trees on hyperbolic disk is the following: if $\tau = \frac{1+\varepsilon}{\varepsilon} \cdot 2 \log \frac{deg_{max}}{\pi/2}$, then the worst-case
distortion is no more than $1 + \varepsilon$.

Their embedding of graphs into weighted trees is based on the paper~\cite{abraham2007reconstructing} which I discuss next.

\paragraph{Reconstructing Approximate Tree Metrics~\cite{abraham2007reconstructing}}

Note: this paper discusses distortion, which is actually worst-case distortion $D_{WC}$.

In another paper it is shown that a metric space is a tree metric if and only if the sub-metric induced by any four points is a tree metric, which is equivalent to the following Four-Points Condition (4PC): for any four points, out of the three possible matchings, the two matchings that have the maximum weight have equal weight.

Also, they refer to Gromov's $\delta$-hyperbolicity, which is the following. A metric space
$\delta$-hyperbolic if for every four points $w,x,y,z$ that are ordered such that $d(w,x) + d(y,z) \le d(w,y) + d(x,z) \le d(w,z) + d(x,y)$, we have $d(w, z) + d(x, y) \le d(w, y) + d(x, z) + \delta$. The $\delta$-hyperbolicity condition is equivalent to assuming that every 4 points embed into a tree metric with additive distortion $O(\delta)$.

In the current paper, the authors propose $\varepsilon-4PC$, where $d(w, z) + d(x, y) \le d(w, y) + d(x, z) + 2\varepsilon\cdot\min{d(w,x),d(y,z)}$. $\varepsilon = 0$ gives 4PC. For $\varepsilon = 1$ the property is always satisfied due to the triangle inequality. The authors argue that from the practical point of view this property is more useful.

They show that of $\varepsilon$-4PC, then there is a constant $c_1$ such that we can embed into a tree metric with a maximum distortion of $(1+\varepsilon)^{c_1 \cdot \log n}$. And also there is a lower bound (for some metric space) with another constant $c_2$.

They give an algorithm for embedding. The algorithm is based on adding Steiner nodes recursively. Note that the distances among every set of three points $x, y, z$ of a metric space can be exactly represented by a tree by adding one additional Steiner node. The Gromov product $(x|y)_z = \frac 1 2 \cdot (d(x,z) + d(y,z) - d(x,y))$ is the distance from such Steiner node to $z$.

\paragraph{Poincar\'e GloVe: Hyperbolic Word Embeddings~\cite{tifrea2018poincar}}

This paper discusses and computes average $\delta$-hyperbolicity, which is discussed in Appendix B. 
Using the notation of the paragraph above, they write that for four point $\delta = (d(w, z) + d(x, y) - d(w, y) - d(x, z))/2$. Then, the supremum of these numbers over all 4-tuples is $\delta_{worst}$.
They (reasonably) argue that the average $\delta_{avg}$ is more meaningful than $\delta_{worst}$.
If we can define triangles, then an equivalent definition is: $\delta_{worst}$ is the smallest $\delta > 0$ such that for any triangle there exists a point at distance at most $\delta$ from each side of the triangle. 
Note that Euclidean space is $\infty$-hyperbolic.
They also report the normalized value $2 \delta_{avg} / d_{avg}$ which is invariant to metric scaling. 
Note that low values of both $\delta_{avg}$ and $d_{avg}$ are indicators of some hyperbolicity. 

\paragraph{Ricci Curvature of the Internet Topology~\cite{ni2015ricci}}

Empirical paper: analyze Ricci curvature of some graphs. Also, analyze computation of Ricci curvature. Show that in networks, the distribution of Ricci curvature is highly
non-homogeneous. And there are many negatively curved edges, but also communities of positive curved edges. The curvature distribution for different
type of Internet topology can be very different. As expected, removing edges of negative curvatures disconnect network fast.

The correlation between Ricci curvature and other properties is shown on Figures 9-13. They consider betweenness centrality, farness centrality, degree and clustering coefficient.

Also, this paper has a nice intuitive introduction into Gromov hyperbolicity and sectional and Ricci curvature. Speaking about Gromov, this paper has yet another definition of this curvature: metric has $\delta$-hyperbolicity if all geodesic triangles are $\delta$-slim, that is, for any three points
$a, b, c$, the geodesic paths (shortest paths) between them satisfy
the following property: any point on one path is within
distance $\delta$ from the closest vertex on the other two paths.s
Further, we can compare $\delta(\Delta abc)$ with
the diameter of the triangle $\Delta abc$. The ratio is bounded above
by 3/2 in a Riemannian manifold of constant nonpositive
curvature, thus, in other papers, it was suggested to use 3/2 as the threshold to
distinguish whether the graph metric is hyperbolic or not.

\paragraph{Upper bound on scaled Gromov-hyperbolic $\delta$~\cite{jonckheere2007upper}}

This paper defines $\delta$ in yet another way and discusses the bound $2/3$ mentioned above. Namely, now $\delta(\Delta ABC)$ is the smallest value of $d(X,Y) + d(Y,Z) + d(Z, X)$, where $X, Y, Z$ are some points on the geodesics forming this triangle. For a tree we have $\delta = 0$. In Eucledian space the points $X, Y, Z$ form orthic triangle.

%\paragraph{What relations are reliably embeddable in Euclidean space?~\cite{bhattacharjee2019relations}}

%About embeddings of directed graphs to euclidean space. Also contains a short background on undirected graphs (section 1.1), in particular cite some known theoretical results on the subject. Also, focus on precision of embeddings. Maybe we can use something as a starting point for the theory. 

%\paragraph{Hyperbolic Entailment Cones for Learning Hierarchical Embeddings~\cite{ganea2018hyperbolic}}

%Propose a new method for embedding directed acyclic graphs (DAGs).  Use nested geodesically convex cones, which improve embeddings in both euclidean and hyperbolic spaces. 

\section{How to estimate global curvature}

\subsection{Number of $k$-neightbors}

Let us take a node $v$. Let $N_k(v)$ denote the number of nodes at distance exactly $k$ from $v$. Assuming that nodes are roughly uniformly distributed, we can compare $N_k(v)$ with a surface area of a ball of radius $k$. 

In euclidean space, surface area of a ball grows as $k^{d-1}$ (up to some multiplicative function of $d$). 

Let us consider spherical space. Here, if I am not mistaken, up to some multiplier depending on $d$, we have  $\left(R \sin \theta\right)^{d-1}$. Curvature is $c = 1/R^2$, radius is $k = R \cdot \theta$, so surface area is $\left(\frac{1}{\sqrt{c}}\sin{\sqrt{c}\,k}\right)^{d-1}$.

Let us consider hyperbolic space. Here, assuming the the curvature is $-c$, we have $\left(\frac{1}{\sqrt{c}}\sinh{\sqrt{c}\,k}\right)^{d-1}$.
%and
%\[
%\sinh{\sqrt{c}\,r} = %\frac{e^{\sqrt{c}\,r}+e^{-\sqrt{c}\,r}}{2}.
%\]

So, the algorithm can be the following (recall that we are given dimentsion $d$): consider $N_k^{\frac 1 {d-1}}$ for some small $k$, check if the obtained function is convex or concave or straight line. Then, fit the corresponding function ($\sin$, $\sinh$), obtain curvature.

\subsection{Modifications of the above idea}

Assumption about uniform distribution is rather strong. In particular, we could take into accound the edges between the vertices of $N_k$. Here I see two options.

The first one is the following: let us take nodes at distance $k$ and consider the maximum independent subset in this set (assume that its size is $N_k'$). Then, we know that for each vertex there are no other vertices in the ball of radius 0.5, which gives us the lower bound for the required volume: $N_k'/2$. So, we can use the algorithm discussed above, but with $N_k'$ instead of $N_k$.

The second one is similar to the previous, but maybe considering the independent set is too restrictive. So, we may look at the number of non-edges between the vertices on distance $k$. If this number if $L$, then we can say that the volume grows as $N_k''$ with $N_k''(N_k''-1)/2 = L$ (no good theory behind that).

\subsection{Intersecting balls}

Consider our graph $G$. Let us take two balls (considering graph distance) at distance $l$ and with radius $k$. I do not have formulas now, but in euclidean/hyperbolic/spherical spaces we can compute the volume of the intersection of such balls relative to the volume of their union. This will be the function of dimension and curvature, dimension is given, so we can use this to estimate the curvature.

I see two weaknesses of this approach: 1) formulas seems to be more complicated, even for euclidean space, 2) it is not clear which radius and which distance to consider, my first idea was to take distance 1 and radius 1 (i.e., analyze the fraction of common neighbors), but this fails on, e.g., integer lattice in euclidean space.


\section{Measuring quality of embeddings}



\paragraph{Mean Average Precision (MAP)}

Let us consider the node $v$. $N_i(v)$ be the number of nodes closest to $i$ we have to consider to cover $i$ neighbors of $i$. Note that in the ideal situation we have $N_i(v) = i$. Then MAP is defined as 
\[
\frac{1}{n} \sum_v \frac{1}{deg(v)} \sum_{i=1}^{deg(v)} \frac{i}{N_i(v)}. 
\]

\paragraph{Distortion} This is a standard metric for graph embeddings.

\[
D(f) = \frac{1}{\binom{n}{2}} \sum_{u \neq v}  \frac{|d(f(u),f(v)) - d_G(u,v)|}{d_G(u,v)}.
\]

There is also a variant called worst-case distortion:
\[
D_{WC}(f) = \frac{\max_{u\neq v}d(f(u),f(v))/d_G(u,v)}{\min_{u\neq v}d(f(u),f(v))/d_G(u,v)} 
\]

\section{Curvature in graphs}

There are many different notions of graphs curvature. Here we describe a few of them, starting with the most prominent one Ollivier-Ricci curvature.

\subsection{Ollivier-Ricci curvature}

To define this curvature we first need to introduce the Wasserstein metric.

Let $(\mathcal{X},d)$ be a metric space (which technically should be Radon or Polish) and consider two probability measures $\mu_1$ and $\mu_2$ on this space. Recall that a coupling between $\mu_1$ and $\mu_2$ is a joint probability measure $\mu$ whose marginals are $\mu_1$ and $\mu_2$. Let $\Gamma(\mu_1, \mu_2)$ denote the set of all such couplings. The Wasserstein metric (Kantorovich-Rubenstein distance of order one) between $\mu_1$ and $\mu_2$ is then given by
\begin{equation}\label{eq:def_wasserstein_inf}
	W_1(\mu_1, \mu_2) = \inf_{\mu \in \Gamma(\mu_1, \mu_2)} \int_{\mathcal{X} \times \mathcal{X}} d(x,y) \, d\mu(x,y)
\end{equation}

Due to a duality theorem by Kantorovich and Rubenstein we have the following equivalent definition of $W_1(\mu_1, \mu_2)$
\begin{equation}\label{eq:def_wasserstein_sup}
	W_1(\mu_1, \mu_2) = \sup_{f \in L_1} \int_\mathcal{X} f(x) \, d\mu_1(x) - \int_\mathcal{X} f(y) \, d\mu_2(y),
\end{equation}
where the supremum is taken over all Lipschitz continuous function on $(X,d)$ with Lipschitz constant $1$, i.e.
\[
	|f(x) - f(y)| \le d(x,y).
\]

For a graph $G$ we consider the shortest path metric on $G$, denoted by $d_G$, and let $W_1^G$ denote the Wasserstein metric with respect to the metric space $(G,d_G)$. Furthermore, for each node $v$ let $m_v$ denote the uniform probability measure on the neighbors of $v$, i.e.
\[
	m_v(u) = \frac{1_{u \sim v}}{\mathrm{deg}(v)},
\]
where $\mathrm{deg}(v)$ denotes the degree of $v$. Then the classic definition of Ollivier-Ricci curvature between two neighboring nodes $v \sim u$ in $G$ is defined as
\begin{equation}\label{eq:def_classic_ollivier_graphs}
	\kappa_G(u,v) = 1 - W_1^G(m_v, m_u).
\end{equation}

It is important to note that Ollivier-Ricci curvature is defined in much more generality in terms of metrics and random walks, see~\cite{ollivier2009ricci}. Thus different version on graphs can be considered. Definition~\eqref{eq:def_classic_ollivier_graphs} corresponds to the classical choices of graph distance and random walk on the graph.

The strength of Ollivier-Ricci curvature lies in the fact that when we consider its continuous version on Riemannian manifolds than this converges to the Ricci curvature, see~\cite[Example 7]{ollivier2009ricci}. It turns out that this also holds when we consider dense random geometric graphs on Riemannian manifolds (upcoming paper). However, for this one needs to deviate from the classical version and consider random-walks on larger neighborhoods. Still these results highlights that Ollivier-Ricci curvature can properly encode the curvature of the underlying manifold. 


Below we list some simple examples of Ollivier-Ricci curvature in graphs.

\paragraph{Circle}

Consider a circle on $n$ nodes and let $v \sim u$ be two neighbors. Then $W_1^G(m_u,m_v) = 1$ and hence $\kappa_G(u,v) = 0$.

\paragraph{Trees}

Consider any tree graph $T$, let $v \sim u$ be two neighbors. Then Proposition 2 in \cite{jost2014ollivier} states that
\[
	\kappa_G(m_x,m_y) = -2\left(1 - \frac{1}{\mathrm{deg}(v)} - \frac{1}{\mathrm{deg}(u)}\right)_+, 
\]
where $(t)_+ = \max\{0,t\}$. In particular, if either $\mathrm{deg}(v) = 1$ or $\mathrm{deg}(u) = 1$ then it follows that $\kappa_G(u,v) = 0$. However, if $\mathrm{deg}(v) \ge 3$ for all nodes $v$ then $\kappa_G(v,u) < 0$. Thus trees are intrinsic examples of negatively curved graphs, according to Ollivier-Ricci curvature.

\paragraph{Lattice}
Consider the $2$-dimensional lattice $\mathbb{Z}^2$ and let $v \sim u$ be two neighbors. Then the optimal transportation from $m_u$ to $m_v$ is given by a translation by the vector $y - x$ and hence $W_1^G(m_v,m_u) = d_G(u,v) = 1$, see also \cite[Example 5]{ollivier2009ricci}, and thus $\kappa_G(u,v) = 0$.

\paragraph{Complete graph}

Consider a complete graph on $n$ nodes. Then, for any two nodes $u$ and $v$ it follows from Example 1 in~\cite{jost2014ollivier} that $\kappa_G(m_v, m_u) = \frac{n-2}{n-1} \to 1$ as $n \to \infty$.

\paragraph{Erd\H{o}s-Renyi graph}

The following results from~\cite{lin2011ricci} characterize Ollivier-Ricci curvature in $G(n,p)$:

\begin{enumerate}
\item If $p \ge \log(n)^{1/3} n^{-1/3}$ then almost surely
\[
	\kappa_G(u,v) = p + O\left(\sqrt{\frac{\log(n)}{n p}}\right).
\]
\item If $2 \log(n)^{1/2} n^{-1/2} \le p < \log(n)^{1/3} n^{-1/3}$ then almost surely
\[
	\kappa_G(u,v) = O\left(\frac{\log(n)}{n p^2}\right).
\]
\item If $\log(n)^{1/2} n^{-2/3} \ll p \ll \log(n)^{-1/2}$ then almost surely
\[
	\kappa_G(u,v) = -1 + O\left(n p^2\right) + O\left(\frac{\log(n)}{n^2 p^3}\right).
\]
\item If $\log(n) n^{-1} \ll p \ll n^{-2/3}$ then almost surely
\[
	\kappa_G(u,v) = -2 + O\left(n^2 p^3\right) + O\left(\sqrt{\frac{\log(n)}{np}}\right)
\]
\end{enumerate}

\subsection{Forman-Ricci curvature}

Another notion of graph curvature called Forman-Ricci curvature~\cite{sreejith2016forman} is based on the discretization of Ricci curvature proposed by Forman~\cite{forman2003bochner}. It is defined for general weighted graph $G$, with both vertex weights $w(v)$ and edge weights $w(u,v)$, as follows
\begin{equation}\label{eq:def_forman_curvature_edges}
	F(u,v) = w(u,v) \left(\frac{w(u) + w(v)}{w(u,v)} - \hspace{-8pt}\sum_{u^\prime \in N(u)\setminus v}  \frac{w(u)}{w(u,u^\prime)} - \hspace{-8pt} \sum_{v^\prime \in N(v) \setminus u} \frac{w(v)}{w(v,v^\prime)}\right),
\end{equation}
where $N(v)$ denotes the neighborhood of a node $v$. A definition of Forman-Ricci curvature for vertices is then obtained by averaging over all neighbors
\begin{equation}\label{eq:def_forman_curvature_nodes}
	F(v) = \frac{1}{\mathrm{deg}(v)} \sum_{u \in N(v)} F(u,v).
\end{equation}

When the graph $G$ is not weighted, i.e. when $w(v) = 1 = w(v,u)$ for all nodes $v$ and edges $(u,v)$, we get
\begin{align*}
	F(u,v) &= \left(2 - (\mathrm{deg}(v) - 1) - (\mathrm{deg}(u) - 1)\right) \\
	&= 4 - (\mathrm{deg}(v) + \mathrm{deg}(u)).
\end{align*}
In particular, Forman-Ricci curvature of a graph is zero if and only if the graph is a circle.

The general formula by Forman included the faces of any dimension. This can of course be done for graphs as well, see~\cite{weber2017coarse}. In~\eqref{eq:def_forman_curvature_edges} only $1$-dimensional faces (edges) are included. However, one can also include $2$-dimensional faces. This was considered in~\cite{samal2018comparative}, where only the $2$-dimensional faces on three nodes (triangles) were considered. In the case of an unweighted graph we then obtain
\begin{equation}\label{eq:def_forman_curvature_triangles}
	F_2(u,v) = F(u,v) + 3\Delta(u,v),
\end{equation}
where $\Delta(u,v)$ is the number of triangles that contain the edge $(u,v)$. This is the same as the number of common neighbors of $u$ and $v$. 

\subsection{Approximation of Ollivier curvature}

An explicit formula for Ollivier-Ricci curvature in graphs is obtained in~\cite{kelly2019self}, under somewhat restrictive assumptions. In general it could be used as an approximation.

For an edge $(u,v)$, let $\triangle_{uv}$, $\square_{uv}$ and $\pentagon_{uv}$ denote, respectively, the number of triangles, squares and pentagons in the graph that include the edge $(u,v)$. Moreover let $a \wedge b = \min\{a,b\}$, $a \vee b = \max\{a,b\}$ and $[a]_+ = a \vee 0$. Then we define:
\begin{equation}\label{eq:def_ollivier_ricci_approx}
	\begin{aligned}
		\kappa_G^\ast(u,v) &= \frac{\triangle_{uv}}{\mathrm{deg}(u) \wedge \mathrm{deg}(v)} 
			- \left[1 - \frac{1 + \triangle_{uv} + \square_{uv}}{\mathrm{deg}(u) \vee \mathrm{deg}(v)} - \frac{1}{\mathrm{deg}(u) \wedge \mathrm{deg}(v)}\right]_+\\
		&\hspace{10pt}- \left(\frac{\triangle_{uv}}{\mathrm{deg}(u) \vee \mathrm{deg}(v)} 
			- \frac{\triangle_{uv}}{\mathrm{deg}(u) \wedge \mathrm{deg}(v)}\right)\\
		&\hspace{25pt}\vee \left(1 - \frac{1 + \triangle_{uv} + \square_{uv} + \pentagon_{uv}}
			{\mathrm{deg}(u) \vee \mathrm{deg}(v)}
			- \frac{1}{\mathrm{deg}(u) \wedge \mathrm{deg}(v)}\right).
	\end{aligned}	
\end{equation}

In~\cite{kelly2019self} it was shown that $\kappa_G^\ast(u,v) = \kappa_G(u,v)$ if and only if short cycles of length $3$,$4$ or $5$ that include the edge $(u,v)$ share no other edges. For example, if $w$ is a neighbor of both $u$ and $v$ (so $w$ is part of a $3$-cycle) then $w$ cannot be connected to any other neighbor of either $u$ or $v$, since that would mean that $w$ is part of a $4$-cycle that includes the edge $(u,v)$. Although this requirement is quite restrictive, one could still use~\eqref{eq:def_ollivier_ricci_approx} as an approximation (or even a definition) of curvature in graphs.

{\bf [By adjusting the definitions used in~\cite{kelly2019self} it might be possible to derive an upper bound for Ollivier curvature in terms of disjoint triangles, squares and pentagrams. It will look very similar to~\eqref{eq:def_ollivier_ricci_approx} and will be exact when the independent short cycle assumption holds.]}

\subsection{Heuristic sectional curvature from~\cite{gu2019learning}}

In~\cite{gu2019learning} a different notion of curvature is used, based on the parallelogram law. This is one of our main baselines, so we should compare with this theoretically and empirically. For a parallelogram with nodes $a,b,c,d$ and sides $ab, ac, bd, cd$, the parallelogram law states that:
\[
	2d(c,d)^2 + 2d(b,d)^2 = d(b,c)^2 + d(a,d)^2.
\]
Now imagine a point $m$ in the center of the line connecting $a$ and $d$, i.e. $d(a,m)=d(a,d)/2=d(m,d)$. Then we have that
\begin{align*}
	4d(a,m)^2 &= d(a,d)^2 = 2d(c,d)^2 + 2d(b,d)^2 - d(b,c)^2 \\
	&= 2d(a,b)^2 + 2d(a,c)^2 - d(b,c)^2,
\end{align*}
where the last line is because $d(a,c) = d(b,d)$ and $d(a,b) = d(c,d)$. This then implies that
\begin{equation}\label{eq:parallelogram_law}
	d(a,m)^2 + \frac{d(b,c)^2}{4} - \frac{d(a,b)^2 + d(a,c)^2}{2} = 0.
\end{equation}

The key intuition behind the definition in~\cite{gu2019learning} is that this holds in flat Euclidean space while in a positively or negative curved space the left hand side in~\eqref{eq:parallelogram_law} should be positive or negative, respectively. 

For graphs, Let $v$ be a node in $G$, $u,w$ neighbors of $v$ and $z$ any other node. Then we define
\begin{equation}
	\xi_G(v,u,w,z) = d(v,z)^2 + \frac{d(u,w)^2}{4} - \frac{d(v,u)^2 + d(v,w)^2}{2}.
\end{equation}
Note that this resembles the left hand side in~\eqref{eq:parallelogram_law} with $a = v$, $b = u$, $c = w$ and $m = z$. Again, based on the parallelogram law, the intuition is that if these four vertices are supposed to be embedded in negative, flat or positive curved space then $\xi_G(v,u,w,z)$ should be negative, zero or positive, respectively. Now consider any node $v$ and two of its neighbors $u$ and $w$. Then we define the graph sectional curvature of a node $v$ and is neighbors $u,w$ as the average of $\xi_G(v,u,w,z)$,
\[
	\xi_G(v; u,w) = \frac{1}{|G|-1} \sum_{z \in G\setminus v} \frac{\xi_G(v,u,w,z)}{2d_G(v,z)}.
\] 
Here the normalization constant $2d_G(v,z)$ is include to yield the right scalings for trees and cycles.

The curvature of the graph is then defined as the average of $\xi_G(v; u,w)$. This is computed as $\xi_G = \frac{\xi_G(v^\ast,u^\ast,w^\ast,z^\ast)}{2d_G(v^\ast,z^\ast)}$, where $v^\ast, u^\ast, w^\ast$ and $z^\ast$ are selected uniformly at random from $G$, $N(v^\ast)$, $N(v^\ast)\setminus u^\ast$ and $G\setminus v^\ast$, respectively. Here $N(v)$ denotes the neighborhood of $v$.



\section{Global graph curvature}

\subsection{Possible definition}
        
Assume that we are given a loss function $L(e(G),G)$ which compares graph $G$ and its embedding. For example, $L$ can be distortion.

Assume that we have some ideal network embedder $e_{d,C}(G)$ which embeds $G$ into a $d$-dimensional space of curvature $C$ and minimizes $L$. Then we can define a $d$-dimensional curvature of $G$ in the following way:
\[
C_d(G) = \textrm{argmin}_C L(e_{d,C}(G),G)\,.
\]

\subsection{Cycle of size $n$}
                     
\paragraph{Ollivier-Ricci curvature} $\kappa_G(u,v) = 0$ for all edges, so by averaging we obtain $0$.

\paragraph{Forman-Ricci curvature} $F(u,v) = 0 = F_2(u,v)$.

\paragraph{Distortion-based curvature} For any dimension $d$ and for $n \ge 4$, there is a curvature which gives zero distortion:
\[
C_d(G) = \left(\frac{2\pi}{n}\right)^2.
\]
This is easy to explain: if we consider three consequent vertices, then the middle one should lie on the geodetic between the other two. So, they all lie on a great circle (of length $n$).

\subsection{Complete graph on $n$ vertices}

\paragraph{Ollivier-Ricci curvature} 
        
$\kappa_G(u,v) = \frac{n-2}{n-1}$, so average is the same.

\paragraph{Forman-Ricci curvature} $F(u,v) = 6 - 2n$, $F_2(u,v) = n$.

\paragraph{Distortion-based curvature} Assume that $d = n-2$, then it is easy to find $C_d(G)$. Note that we want to get $n-1$-simplex, which can be embedded into a $n-2$-dimensional spherical space, the radius is $R = a\sqrt{\frac{n-1}{2n}}$, and we want
\[
2 R \arcsin \frac{a}{2R} = 1.
\]
\[
2 a\sqrt{\frac{n-1}{2n}} \arcsin \sqrt{\frac{n}{2(n-1)}} = 1.
\]
\[
a  = \sqrt{\frac{2n}{(n-1)}}\frac{1}{2\arcsin \sqrt{\frac{n}{2(n-1)}}}.
\]
\[
R = \frac{1}{2\arcsin \sqrt{\frac{n}{2(n-1)}}}
\]

so we have
\[
C_{n-2}(G) = 4 \left(\arcsin \sqrt{\frac{n}{2(n-1)}}\right)^2\,.
\]



\textbf{TODO: analyze smaller $d$}

\subsection{Star on $n$ vertices}

\paragraph{Ollivier-Ricci curvature} $\kappa_G = 0$

\paragraph{Forman-Ricci curvature} $F = 3-n = F_2$

\paragraph{Distortion-based curvature} It seems that in order to minimize distortion, we want $C_d(G) \to -\infty$.
    
\section{Some ideas for global curvature estimation}

\begin{enumerate}
    \item Use the number of neighbors and clustering coefficient. Thus, we can estimate the volume of a ball of a unit radius. 
    \item Use the number of $k$-neighbors (the vertices at distance exactly $k$), so we can understand the speed of growth.
    \item Compare the volume of intersection of two balls with their union. This can also help in estimating dimension.
\end{enumerate}
            
\section{Distortion in Random Geometric Graphs}

Consider the random graph $G_n^r$ constructed by choosing $n$ points uniformly in the square $[\sqrt{n}/2, \sqrt{n}/2]$ and connecting two points $u$ and $v$ if $d(u,v) \le r$. For the remainder of this section we will assume that $r \ge 244 \sqrt{\log(n)}$, which is slightly larger than the threshold value $r_c = \sqrt{\log(n)/\pi}$ at which point the graph $G_n^r$ becomes connected a.a.s. and furthermore that $r \le \sqrt{n}/2$. We will identify nodes $v \in V$ with points and let $(x_v,y_v)$ denote their corresponding coordinates.

Clearly, the structure of the graph $G_n^r$ is completely determined by the coordinates $\{(x_v,y_v)\}_{v \in V}$ of the chosen points. Hence, the most natural and optimal embedding for this graph should be in the $2$-dimensional Euclidean plane using these original coordinates. However, if we do this then the distortion actually diverges.

We use the following result from~\cite{diaz2016relation} concerning the stretch in $G_n^r$:

\begin{theorem}\label{thm:stretch_rgg}
Let $G_n^r$ be constructed as above. Then, for each pair of nodes $u,v \in V$ with $d(u, v) > r$, there exists sequences $a_n^-, a_n^+ \to 0$, depending on $r$, such that the following holds with probability $1 - o(n^{-5/2})$:
\begin{enumerate}
\item If $d(u,v) \ge \max\{12\log(n)^{3/2}/r, 21 r \log(n)\}$:
\[
	d_G(u,v) \ge \left\lfloor \frac{d(u,v)}{r}\left(1 + a_n^-\right)\right\rfloor
\]
\item if $r \ge 224 \sqrt{\log(n)}$:
\[
	d_G(u,v) \le \left\lceil \frac{d(u,v)}{r}\left(1 + a_n^+\right)\right\rceil
\]
\end{enumerate}
\end{theorem}

\begin{lemma}
Let $f : V \to \mathbb{R}^2$ denote the embedding for the graph $G_n^r$ that assigns to each node $v$ it original coordinates $(x_v,y_v)$ and let $D_n(r)$ denote the distortion associated with this embedding. Then
\[
	\mathbb{E}[D_n(r)] = \Omega\left(\sqrt{\log(n)}\right).
\]
\end{lemma}

\begin{proof}
Let $S_n$ denote the event for which the statements of Theorem~\ref{thm:stretch_rgg} holds for all pairs $(u,v)$. Then since the probability that it holds for one pair is $1 - o(n^{-5/2})$, it follows that $\mathbb{P}(S_n) = 1 - o(n^{-1/2})$.

On the event $S_n$, it follows from the second statement of Theorem~\ref{thm:stretch_rgg} that for all $u,v$ with $d(u,v) > r$
\[
	\left|\frac{d(u,v)}{d_G(u,v)} - 1\right| \ge \frac{d(u,v)}{d_G(u,v)} - 1
	\ge \frac{r(d_G(u,v) - 1)}{d_G(u,v)(1+a_n^+)} - 1.
\]
Therefore, on the event $S_n$, we have that
\begin{align*}
	D_n(r) &= \frac{1}{\binom{n}{2}} \sum_{u \ne v} \left|\frac{d(u,v)}{d_G(u,v)} - 1\right|\\
	&\ge \frac{1}{\binom{n}{2}} \sum_{u \ne v \atop d(u,v) > r} \left|\frac{d(u,v)}{d_G(u,v)} - 1\right|\\
	&\ge \frac{1}{\binom{n}{2}} \sum_{u \ne v \atop d(u,v) > r} \left(\frac{r(d_G(u,v) - 1)}{d_G(u,v)(1+a_n^+)} - 1\right)\\
	&\ge \left(\frac{r}{1 + a_n^+} - \frac{r}{d_G(u,v)(1+a_n^+)}\right) 
		\frac{|\{u \ne v \, :\, d(u,v) > r\}|}{\binom{n}{2}} - 1\\
	&\ge \left(\frac{r}{1 + a_n^+} - \frac{r}{n(1+a_n^+)}\right) 
			\frac{|\{u \ne v \, :\, d(u,v) > r\}|}{\binom{n}{2}} - 1\\
	&\ge 224 \sqrt{\log(n)} \frac{1 - \frac{1}{n}}{1 + a_n^+} \frac{|\{u \ne v \, :\, d(u,v) > r\}|}{\binom{n}{2}} - 1.
\end{align*}
Next we note that since $224 \pi \log(n) \le \pi r^2 \le \pi n/4$, we have that the expected number of nodes at least a distance $r$ away from a given node is at least $(n - \pi r^2) = \Omega(n)$ (center) and at most $n - \pi r^2/4 = O(n)$ (the corners). We conclude that
\[
	\mathbb{E}[|\{u \ne v \, :\, d(u,v) > r\}|] = \Theta\left(\binom{n}{2}\right).
\]

The result now follows since
\begin{align*}
	\mathbb{E}[D_n(r)] &\ge \mathbb{E}[D_n(r) 1_{S_n}] \\
	&\ge 224 \sqrt{\log(n)} \frac{1 - \frac{1}{n}}{1 + a_n^+} 
		\frac{\mathbb{E}[|\{u \ne v \, :\, d(u,v) > r\}|]}{\binom{n}{2}}
		= \Omega(\sqrt{\log(n)}).
\end{align*}
\end{proof}


\section{Practical tasks}

\subsection{Network reconstruction and link prediction}

\cite{goyal2018graph} uses some standard complex networks: Blogcatalog, YouTube, HepTh, AstroPh, protein-protein interactions. As metrics they use Precision at $k$ and Mean Average Precision (MAP).

\cite{nickel2017poincare} uses the following networks: AstroPh, CondMat, GrQc, HepPh. Each dataset is split into train, validation, and test sets. Parameters are tuned on validation, MAP is measured on test. 

\cite{nickel2017poincare} also uses transitive closure of the WordNet noun hierarchy. Tasks: reconstruction and link prediction. Measure mean rank and (MAP).  The same dataset is used in~\cite{sala2018representation}, where MAP is measured. Similarly, \cite{ganea2018hyperbolic} uses this, treats link prediction as classifiaction task and measures precision, recall, F1.

\cite{sala2018representation} also uses fully-balanced trees along with phylogenetic trees expressing genetic heritage (of mosses growing in urban
environments), a graph of Ph.D. advisor-advisee relationships, biological sets involving disease relationships, protein interactions in yeast bacteria, collaboration network Gr-QC. Measure MAP and distortion. 

\cite{nickel2018learning} uses the following taxonomies: WordNet (noun and verb hierarchy), EuroVoc, ACM, MeSH. Measures MR and MAP. In addition, they also evaluate how well the norm of the embeddings  correlates with
the ground-truth ranks in the embedded taxonomy: they measure the Spearman rank-order correlation of the normalized rank with the norm of the embedding. 
 
\cite{nickel2018learning} also embeds Enron Email Corpus and measures Spearman correlation of the norms of the embedding with
the organizational rank.

 
\subsection{Lexical Entailment}

\cite{nickel2017poincare} uses HyperLex datasets. Uses embedding of WordNet. Records Spearmanâ€™s rank correlation
with the ground-truth ranking. 

\subsection{Node classification}

\cite{goyal2018graph} uses Blogcatalog and protein-protein interactions. Use embeddings as features (as input to a one-vs-rest logistic regression).
	


\bibliographystyle{plain}
\bibliography{references}
\end{document}
