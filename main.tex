\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{url}

\title{Embedding project}
\author{}
\date{}

\usepackage{natbib}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{wasysym}

\begin{document}

\maketitle

\section{Our initial idea (12.06.2019)}

Motivation:
\begin{itemize}
    \item Somehow use our intuition that networks are usually formed from different clusters, which also hierarchically consist of smaller clusters;
    \item Try to formalize a notion of varying dimension: some parts of a graph can be more complex than others, so we need more dimensions for them;
    \item Also try to formalize a notion of varying curvature, in order to be not limited by a fixed one.
\end{itemize}
A possible algorithm:
\begin{itemize}
    \item Consider the whole graph, choose appropriate curvature and dimension, embed a graph into the corresponding space;
    \item Compute ``residual'' graph, find its subgraph with largers error and approximately constant curvature, embed it into the corresponding space;
    \item Continue the process with other subsets;
    \item When compute the distance between two nodes, use its common subspace.
\end{itemize}
Some comments and open questions:
\begin{itemize}
    \item Global curvature of a graph is not defined and studied. It would be interesting to define it based on some practical metric. (And it is not clear how it would be connected with local curvatures of nodes/edges);
    \item Intuitively, it seems to be desirable to move from larger subsets of nodes in the algorithm to smaller ones;
    \item Maybe we can use community detection at some point of the algorithm.
\end{itemize}

\section{Our current simple plan (28.06.2019)}

We think that we will start our experiment with the following simple approach, in order to see whether we can benefit from our general idea.

\begin{itemize}
    \item Take our graph and apply a community detection algorithm to it. There are two options which we discussed: 1) apply some hierarchical algorithm in order to get several levels of granularity as an output; 2) get only one level of communities, but with some overlaps. The further ideas can be applied to both cases, but we like the second one more, so, let me describe this one. 
    \item Assume that we have some budget for the overall dimension. Then, we split this budget into two parts. The first we use to embed the whole graph, using some standard graph embedding approach (using an existing algorithm to choose the curvature). The second part we use in the following way: we embed each cluster separately in its own space (choosing curvature individually for clusters).
    \item When we have to measure the distance between two elements, if the belong to the same cluster, then we use the corresponding cluster embedding, if they belong to different clusters, then we use the embedding of the whole graph.
    \item One more idea that we discussed: when we embed the whole graph we care more about long distances, since short ones will be improved at the cluster embedding stage. Therefore, in the learning procedure we can sample only the pairs which are far away from each other.
\end{itemize}

Speaking about the practical realization, we decided to do the following: 
\begin{itemize}
    \item We want to improve the results of \cite{gu2019learning} using their algorithm and their datasets.
    \item We take the code from their github: \url{https://github.com/HazyResearch/hyperbolics}.
    \item Using this, it seems to be fairly easy to make some starting experiments.
\end{itemize}

Also, we discussed the following possible directions for theoretical (or algorithmic) research:
\begin{itemize}
    \item As we discussed before, it would be very useful to estimate optimal curvature and dimension given a graph (optimal in terms of some target metric, e.g., distortion).
    \item It would be interesting to understand the optimal way to split the overall maximum dinension into two parts: for global graph embedding and local cluster embeddings. On the one hand, global graph embedding is expected to be coarser. On the other hand, the number of vertices in the whole graph is larger. 
    \item Probably unrelated note: it is always useful to understand which algorithm is better at which distances (longer or shorter ones).
\end{itemize}

\section{Measuring quality of embeddings}



\paragraph{Mean Average Precision (MAP)}

Let us consider the node $v$. $N_i(v)$ be the number of nodes closest to $i$ we have to consider to cover $i$ neighbors of $i$. Note that in the ideal situation we have $N_i(v) = i$. Then MAP is defined as 
\[
\frac{1}{n} \sum_v \frac{1}{deg(v)} \sum_{i=1}^{deg(v)} \frac{i}{N_i(v)}. 
\]

\paragraph{Distortion} This is a standard metric for graph embeddings.

\[
D(f) = \frac{1}{\binom{n}{2}} \sum_{u \neq v}  \frac{|d(f(u),f(v)) - d_G(u,v)|}{d_G(u,v)}.
\]

There is also a variant called worst-case distortion:
\[
D_{WC}(f) = \frac{\max_{u\neq v}d(f(u),f(v))/d_G(u,v)}{\min_{u\neq v}d(f(u),f(v))/d_G(u,v)} 
\]

\section{Curvature in graphs}

There are many different notions of graphs curvature. Here we describe a few of them, starting with the most prominent one Ollivier-Ricci curvature.

\subsection{Ollivier-Ricci curvature}

To define this curvature we first need to introduce the Wasserstein metric.

Let $(\mathcal{X},d)$ be a metric space (which technically should be Radon or Polish) and consider two probability measures $\mu_1$ and $\mu_2$ on this space. Recall that a coupling between $\mu_1$ and $\mu_2$ is a joint probability measure $\mu$ whose marginals are $\mu_1$ and $\mu_2$. Let $\Gamma(\mu_1, \mu_2)$ denote the set of all such couplings. The Wasserstein metric (Kantorovich-Rubenstein distance of order one) between $\mu_1$ and $\mu_2$ is then given by
\begin{equation}\label{eq:def_wasserstein_inf}
	W_1(\mu_1, \mu_2) = \inf_{\mu \in \Gamma(\mu_1, \mu_2)} \int_{\mathcal{X} \times \mathcal{X}} d(x,y) \, d\mu(x,y)
\end{equation}

Due to a duality theorem by Kantorovich and Rubenstein we have the following equivalent definition of $W_1(\mu_1, \mu_2)$
\begin{equation}\label{eq:def_wasserstein_sup}
	W_1(\mu_1, \mu_2) = \sup_{f \in L_1} \int_\mathcal{X} f(x) \, d\mu_1(x) - \int_\mathcal{X} f(y) \, d\mu_2(y),
\end{equation}
where the supremum is taken over all Lipschitz continuous function on $(X,d)$ with Lipschitz constant $1$, i.e.
\[
	|f(x) - f(y)| \le d(x,y).
\]

For a graph $G$ we consider the shortest path metric on $G$, denoted by $d_G$, and let $W_1^G$ denote the Wasserstein metric with respect to the metric space $(G,d_G)$. Furthermore, for each node $v$ let $m_v$ denote the uniform probability measure on the neighbors of $v$, i.e.
\[
	m_v(u) = \frac{1_{u \sim v}}{\mathrm{deg}(v)},
\]
where $\mathrm{deg}(v)$ denotes the degree of $v$. Then the classic definition of Ollivier-Ricci curvature between two neighboring nodes $v \sim u$ in $G$ is defined as
\begin{equation}\label{eq:def_classic_ollivier_graphs}
	\kappa_G(u,v) = 1 - W_1^G(m_v, m_u).
\end{equation}

It is important to note that Ollivier-Ricci curvature is defined in much more generality in terms of metrics and random walks, see~\cite{ollivier2009ricci}. Thus different version on graphs can be considered. Definition~\eqref{eq:def_classic_ollivier_graphs} corresponds to the classical choices of graph distance and random walk on the graph.

The strength of Ollivier-Ricci curvature lies in the fact that when we consider its continuous version on Riemannian manifolds than this converges to the Ricci curvature, see~\cite[Example 7]{ollivier2009ricci}. It turns out that this also holds when we consider dense random geometric graphs on Riemannian manifolds (upcoming paper). However, for this one needs to deviate from the classical version and consider random-walks on larger neighborhoods. Still these results highlights that Ollivier-Ricci curvature can properly encode the curvature of the underlying manifold. 


Below we list some simple examples of Ollivier-Ricci curvature in graphs.

\paragraph{Circle}

Consider a circle on $n$ nodes and let $v \sim u$ be two neighbors. Then $W_1^G(m_u,m_v) = 1$ and hence $\kappa_G(u,v) = 0$.

\paragraph{Trees}

Consider any tree graph $T$, let $v \sim u$ be two neighbors. Then Proposition 2 in \cite{jost2014ollivier} states that
\[
	\kappa_G(m_x,m_y) = -2\left(1 - \frac{1}{\mathrm{deg}(v)} - \frac{1}{\mathrm{deg}(u)}\right)_+, 
\]
where $(t)_+ = \max\{0,t\}$. In particular, if either $\mathrm{deg}(v) = 1$ or $\mathrm{deg}(u) = 1$ then it follows that $\kappa_G(u,v) = 0$. However, if $\mathrm{deg}(v) \ge 3$ for all nodes $v$ then $\kappa_G(v,u) < 0$. Thus trees are intrinsic examples of negatively curved graphs, according to Ollivier-Ricci curvature.

\paragraph{Lattice}
Consider the $2$-dimensional lattice $\mathbb{Z}^2$ and let $v \sim u$ be two neighbors. Then the optimal transportation from $m_u$ to $m_v$ is given by a translation by the vector $y - x$ and hence $W_1^G(m_v,m_u) = d_G(u,v) = 1$, see also \cite[Example 5]{ollivier2009ricci}, and thus $\kappa_G(u,v) = 0$.

\paragraph{Complete graph}

Consider a complete graph on $n$ nodes. Then, for any two nodes $u$ and $v$ it follows from Example 1 in~\cite{jost2014ollivier} that $\kappa_G(m_v, m_u) = \frac{n-2}{n-1} \to 1$ as $n \to \infty$.

\paragraph{Erd\H{o}s-Renyi graph}

The following results from~\cite{lin2011ricci} characterize Ollivier-Ricci curvature in $G(n,p)$:

\begin{enumerate}
\item If $p \ge \log(n)^{1/3} n^{-1/3}$ then almost surely
\[
	\kappa_G(u,v) = p + O\left(\sqrt{\frac{\log(n)}{n p}}\right).
\]
\item If $2 \log(n)^{1/2} n^{-1/2} \le p < \log(n)^{1/3} n^{-1/3}$ then almost surely
\[
	\kappa_G(u,v) = O\left(\frac{\log(n)}{n p^2}\right).
\]
\item If $\log(n)^{1/2} n^{-2/3} \ll p \ll \log(n)^{-1/2}$ then almost surely
\[
	\kappa_G(u,v) = -1 + O\left(n p^2\right) + O\left(\frac{\log(n)}{n^2 p^3}\right).
\]
\item If $\log(n) n^{-1} \ll p \ll n^{-2/3}$ then almost surely
\[
	\kappa_G(u,v) = -2 + O\left(n^2 p^3\right) + O\left(\sqrt{\frac{\log(n)}{np}}\right)
\]
\end{enumerate}

\subsection{Forman-Ricci curvature}

Another notion of graph curvature called Forman-Ricci curvature~\cite{sreejith2016forman} is based on the discretization of Ricci curvature proposed by Forman~\cite{forman2003bochner}. It is defined for general weighted graph $G$, with both vertex weights $w(v)$ and edge weights $w(u,v)$, as follows
\begin{equation}\label{eq:def_forman_curvature_edges}
	F(u,v) = w(u,v) \left(\frac{w(u) + w(v)}{w(u,v)} - \hspace{-8pt}\sum_{u^\prime \in N(u)\setminus v}  \frac{w(u)}{w(u,u^\prime)} - \hspace{-8pt} \sum_{v^\prime \in N(v) \setminus u} \frac{w(v)}{w(v,v^\prime)}\right),
\end{equation}
where $N(v)$ denotes the neighborhood of a node $v$. A definition of Forman-Ricci curvature for vertices is then obtained by averaging over all neighbors
\begin{equation}\label{eq:def_forman_curvature_nodes}
	F(v) = \frac{1}{\mathrm{deg}(v)} \sum_{u \in N(v)} F(u,v).
\end{equation}

When the graph $G$ is not weighted, i.e. when $w(v) = 1 = w(v,u)$ for all nodes $v$ and edges $(u,v)$, we get
\begin{align*}
	F(u,v) &= \left(2 - (\mathrm{deg}(v) - 1) - (\mathrm{deg}(u) - 1)\right) \\
	&= 4 - (\mathrm{deg}(v) + \mathrm{deg}(u)).
\end{align*}
In particular, Forman-Ricci curvature of a graph is zero if and only if the graph is a circle.

The general formula by Forman included the faces of any dimension. This can of course be done for graphs as well, see~\cite{weber2017coarse}. In~\eqref{eq:def_forman_curvature_edges} only $1$-dimensional faces (edges) are included. However, one can also include $2$-dimensional faces. This was considered in~\cite{samal2018comparative}, where only the $2$-dimensional faces on three nodes (triangles) were considered. In the case of an unweighted graph we then obtain
\begin{equation}\label{eq:def_forman_curvature_triangles}
	F_2(u,v) = F(u,v) + 3\Delta(u,v),
\end{equation}
where $\Delta(u,v)$ is the number of triangles that contain the edge $(u,v)$. This is the same as the number of common neighbors of $u$ and $v$. 

\subsection{Approximation of Ollivier curvature}

An explicit formula for Ollivier-Ricci curvature in graphs is obtained in~\cite{kelly2019self}, under somewhat restrictive assumptions. In general it could be used as an approximation.

For an edge $(u,v)$, let $\triangle_{uv}$, $\square_{uv}$ and $\pentagon_{uv}$ denote, respectively, the number of triangles, squares and pentagons in the graph that include the edge $(u,v)$. Moreover let $a \wedge b = \min\{a,b\}$, $a \vee b = \max\{a,b\}$ and $[a]_+ = a \vee 0$. Then we define:
\begin{equation}\label{eq:def_ollivier_ricci_approx}
	\begin{aligned}
		\kappa_G^\ast(u,v) &= \frac{\triangle_{uv}}{\mathrm{deg}(u) \wedge \mathrm{deg}(v)} 
			- \left[1 - \frac{1 + \triangle_{uv} + \square_{uv}}{\mathrm{deg}(u) \vee \mathrm{deg}(v)} - \frac{1}{\mathrm{deg}(u) \wedge \mathrm{deg}(v)}\right]_+\\
		&\hspace{10pt}- \left(\frac{\triangle_{uv}}{\mathrm{deg}(u) \vee \mathrm{deg}(v)} 
			- \frac{\triangle_{uv}}{\mathrm{deg}(u) \wedge \mathrm{deg}(v)}\right)\\
		&\hspace{25pt}\vee \left(1 - \frac{1 + \triangle_{uv} + \square_{uv} + \pentagon_{uv}}
			{\mathrm{deg}(u) \vee \mathrm{deg}(v)}
			- \frac{1}{\mathrm{deg}(u) \wedge \mathrm{deg}(v)}\right).
	\end{aligned}	
\end{equation}

In~\cite{kelly2019self} it was shown that $\kappa_G^\ast(u,v) = \kappa_G(u,v)$ if and only if short cycles of length $3$,$4$ or $5$ that include the edge $(u,v)$ share no other edges. For example, if $w$ is a neighbor of both $u$ and $v$ (so $w$ is part of a $3$-cycle) then $w$ cannot be connected to any other neighbor of either $u$ or $v$, since that would mean that $w$ is part of a $4$-cycle that includes the edge $(u,v)$. Although this requirement is quite restrictive, one could still use~\eqref{eq:def_ollivier_ricci_approx} as an approximation (or even a definition) of curvature in graphs.

\subsection{Heuristic sectional curvature from~\cite{gu2019learning}}

\textbf{[To be described]}

This is one of our main baselines, so we should compare with this theoretically and empirically.


\section{Global graph curvature}

\subsection{Possible definition}
        
Assume that we are given a loss function $L(e(G),G)$ which compares graph $G$ and its embedding. For example, $L$ can be distortion.

Assume that we have some ideal network embedder $e_{d,C}(G)$ which embeds $G$ into a $d$-dimensional space of curvature $C$ and minimizes $L$. Then we can define a $d$-dimensional curvature of $G$ in the following way:
\[
C_d(G) = \textrm{argmin}_C L(e_{d,C}(G),G)\,.
\]

\subsection{Circle of size $n$}
                     
\paragraph{Ollivier-Ricci curvature} $\kappa_G(u,v) = 0$ for all edges, so by averaging we obtain $0$.

\paragraph{Forman-Ricci curvature} $F(u,v) = 0 = F_2(u,v)$.

\paragraph{Distortion-based curvature} For any dimension $d$ and for $n \ge 4$, there is a curvature which gives zero distortion:
\[
C_d(G) = \frac{2\pi}{n}.
\]
This is easy to explain: if we consider three consequent vertices, then the middle one should lie on the geodetic between the other two. So, they all lie on a great circle (of length $n$).

\subsection{Complete graph on $n$ vertices}

\paragraph{Ollivier-Ricci curvature} 
        
$\kappa_G(u,v) = \frac{n-2}{n-1}$, so average is the same.

\paragraph{Forman-Ricci curvature} $F(u,v) = 6 - 2n$, $F_2(u,v) = n$.

\paragraph{Distortion-based curvature} Assume that $d = n-2$, then it is easy to find $C_d(G)$. Note that we want to get $n-1$-simplex, which can be embedded into a $n-2$-dimensional spherical space, the radius is $R = \sqrt{\frac{n-1}{2n}}$, so we have
\[
C_{n-2}(G) = \sqrt{\frac{2n}{n-1}}\,.
\]
\textbf{TODO: analyze smaller $d$}

\subsection{Star on $n$ vertices}

\paragraph{Ollivier-Ricci curvature} $\kappa_G = 0$

\paragraph{Forman-Ricci curvature} $F = 3-n = F_2$

\paragraph{Distortion-based curvature} It seems that in order to minimize distortion, we want $C_d(G) \to -\infty$.
    
\section{Some ideas for global curvature estimation}

\begin{enumerate}
    \item Use the number of neighbors and clustering coefficient. Thus, we can estimate the volume of a ball of a unit radius. 
    \item Use the number of $k$-neighbors (the vertices at distance exactly $k$), so we can understand the speed of growth.
    \item Compare the volume of intersection of two balls with their union. This can also help in estimating dimension.
\end{enumerate}
            
\section{Practical tasks}

\subsection{Network reconstruction and link prediction}

\cite{goyal2018graph} uses some standard complex networks: Blogcatalog, YouTube, HepTh, AstroPh, protein-protein interactions. As metrics they use Precision at $k$ and Mean Average Precision (MAP).

\cite{nickel2017poincare} uses the following networks: AstroPh, CondMat, GrQc, HepPh. Each dataset is split into train, validation, and test sets. Parameters are tuned on validation, MAP is measured on test. 

\cite{nickel2017poincare} also uses transitive closure of the WordNet noun hierarchy. Tasks: reconstruction and link prediction. Measure mean rank and (MAP).  The same dataset is used in~\cite{sala2018representation}, where MAP is measured. Similarly, \cite{ganea2018hyperbolic} uses this, treats link prediction as classifiaction task and measures precision, recall, F1.

\cite{sala2018representation} also uses fully-balanced trees along with phylogenetic trees expressing genetic heritage (of mosses growing in urban
environments), a graph of Ph.D. advisor-advisee relationships, biological sets involving disease relationships, protein interactions in yeast bacteria, collaboration network Gr-QC. Measure MAP and distortion. 

\cite{nickel2018learning} uses the following taxonomies: WordNet (noun and verb hierarchy), EuroVoc, ACM, MeSH. Measures MR and MAP. In addition, they also evaluate how well the norm of the embeddings  correlates with
the ground-truth ranks in the embedded taxonomy: they measure the Spearman rank-order correlation of the normalized rank with the norm of the embedding. 
 
\cite{nickel2018learning} also embeds Enron Email Corpus and measures Spearman correlation of the norms of the embedding with
the organizational rank.

 
\subsection{Lexical Entailment}

\cite{nickel2017poincare} uses HyperLex datasets. Uses embedding of WordNet. Records Spearman’s rank correlation
with the ground-truth ranking. 

\subsection{Node classification}

\cite{goyal2018graph} uses Blogcatalog and protein-protein interactions. Use embeddings as features (as input to a one-vs-rest logistic regression).

\section{Selected relevant papers}

\paragraph{Graph embedding techniques, applications, and performance: A survey~\cite{goyal2018graph}}

A survey of graph embedding techniques, covers many different methods, but all based on euclidean space (with euclidean distance or dot product). The code is available at \url{https://github.com/palash1992/GEM}.

\paragraph{What relations are reliably embeddable in Euclidean space?~\cite{bhattacharjee2019relations}}

About embeddings of directed graphs to euclidean space. Also contains a short background on undirected graphs (section 1.1), in particular cite some known theoretical results on the subject. Also, focus on precision of embeddings. Maybe we can use something as a starting point for the theory. 

\paragraph{Poincare embeddings for learning hierarchical representations~\cite{nickel2017poincare}}

Influential paper - they were the first to use hyperbolic embeddings in computer science.
Embed in multidimensional Poincar{\'e} ball. Propose embedding algorithm based on Riemannian optimization. The code is available at \url{https://github.com/facebookresearch/poincare-embeddings}.

\paragraph{Learning Continuous Hierarchies in the Lorentz Model of Hyperbolic Geometry~\cite{nickel2018learning}}

Follow-up for the previous work, show that it is better to embed to Lorentx model of hyperbolic space. 

\paragraph{Learning Mixed-Curvature Representations in Product Spaces~\cite{gu2019learning}}

Embed graphs to a product manifold combining multiple copies of spherical,
hyperbolic, and Euclidean spaces. The code is available at \url{https://github.com/HazyResearch/hyperbolics}

\paragraph{Representation Tradeoffs for Hyperbolic Embeddings~\cite{sala2018representation}}

Interesting observation that not only dimensionality, but also precision is important.
Propose a new algorithm for graph embedding: first, embed a graph into a weighted tree, and then embed that tree into the hyperbolic disk. The code is available at \url{https://github.com/HazyResearch/hyperbolics}.

\paragraph{Hyperbolic Entailment Cones for Learning Hierarchical Embeddings~\cite{ganea2018hyperbolic}}

Propose a new method for embedding directed acyclic graphs (DAGs). 
Use nested geodesically convex cones, which improve embeddings in both euclidean and hyperbolic spaces. 

\bibliographystyle{plain}
\bibliography{references}
\end{document}
