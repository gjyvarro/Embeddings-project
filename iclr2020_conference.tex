\documentclass{article} % For LaTeX2e
\usepackage{iclr2020_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{wasysym}
\usepackage{amsthm}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{assumption}[definition]{Assumption}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{remark}[theorem]{Remark}


\title{Global graph curvature}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.  Funding acknowledgements go at the end of the paper.} \\
Department of Computer Science\\
Cranberry-Lemon University\\
Pittsburgh, PA 15213, USA \\
\texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
\And
Ji Q. Ren \& Yevgeny LeNet \\
Department of Computational Neuroscience \\
University of the Witwatersrand \\
Joburg, South Africa \\
\texttt{\{robot,net\}@wits.ac.za} \\
\AND
Coauthor \\
Affiliation \\
Address \\
\texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Recently, non-Euclidean spaces became popular for embedding structured data. However, determining the suitable curvature for a given dataset is still an open problem. 
In this paper, we define a notion of global graph curvature and analyze a problem of estimating this curvature using only graph-based characteristics (without actual embedding). 
We review the existing notions of local curvature (e.g., Ollivier-Ricci curvature) and show that they are often unable to estimate the global one. 
Hence, we propose a new estimator of global graph curvature which increases the quality of graph embeddings.
\end{abstract}

\section{Introduction}

\textbf{[To be written later]}

\section{Background}

\paragraph{Embeddings}

\paragraph{Hyperbolic and Spherical Spaces}

\section{Local graph curvatures}

There are several different notions of local graph curvature proposed in the literature. Many of them are based on the notion of sectional curvature and Ricci curvature defined for Riemannian manifolds. Intuitively, Ricci curvature controls whether the distance between small spheres is smaller or larger than the distance between the centers of the spheres. 
Ricci curvature is positive if
small spheres are closer than their centers are.
%Ricci curvature can also be understood as representing the amount by which the volume of a geodesic ball in a curved Riemannian manifold deviates from that of the standard ball in Euclidean space. 
%If Ricci curvature at a point is non-negative, the volume growth with respect to the radius of the ball centered at this point is a polynomial; if the Ricci curvature is negative everywhere, the volume growth is exponential. 
\textbf{[Probably rewrite and give a reference to the formal definition of Ricci curvature]}

\subsection{Ollivier curvature}

Ollivierâ€™s coarse Ricci curvature translates the
definition of Ricci curvature to graph. 
Again, the idea is to compare the distances between two small balls with the
distance between their centers. 
The distance between balls is defined by the well known optimal transport
distance (a.k.a. Wasserstein distance, earth-mover distance,
Monge-Kantorovich-Rubinstein distance).

Formally, for a graph $G$ we consider the shortest path metric on $G$, denoted by $d_G$, and let $W_1^G$ denote the Wasserstein metric with respect to the metric space $(G,d_G)$. Furthermore, for each node $v$ let $m_v$ denote the uniform probability measure on the neighbors of $v$, i.e., $m_v(u) = \frac{1_{u \sim v}}{\mathrm{deg}(v)}$, where $\mathrm{deg}(v)$ denotes the degree of $v$. Then, the classic definition of Ollivier curvature between two neighboring nodes $v \sim u$ in $G$ is defined as
\begin{equation}\label{eq:def_classic_ollivier_graphs}
	\kappa_G(u,v) = 1 - W_1^G(m_v, m_u).
\end{equation}

It is important to note that Ollivier curvature is defined in much more generality in terms of metrics and random walks, see~\cite{ollivier2009ricci}. Thus, different version on graphs can be considered. \Eqref{eq:def_classic_ollivier_graphs} corresponds to the classical choices of graph distance and random walk on the graph.

\textbf{[Should we discuss the convergence of Ollivier curvature to the curvature of the underlying manifold? Let us discuss whether it can strengthen the paper or not (probably not).]}
%The strength of Ollivier-Ricci curvature lies in the fact that when we consider its continuous version on Riemannian manifolds than this converges to the Ricci curvature, see~\cite[Example 7]{ollivier2009ricci}. It turns out that this also holds when we consider dense random geometric graphs on Riemannian manifolds (upcoming paper). However, for this one needs to deviate from the classical version and consider random-walks on larger neighborhoods. Still these results highlights that Ollivier-Ricci curvature can properly encode the curvature of the underlying manifold. 



\subsection{Forman curvature}

Another notion of graph curvature called Forman curvature~\citep{sreejith2016forman} is based on the discretization of Ricci curvature proposed by~\citet{forman2003bochner}. It is defined for general weighted graph $G$, with both vertex weights $w(v)$ and edge weights $w(u,v)$, as follows
\begin{equation}\label{eq:forman}
	F_G(u,v) = w(u,v) \left(\frac{w(u) + w(v)}{w(u,v)} - \hspace{-8pt}\sum_{u^\prime \in N(u)\setminus v}  \frac{w(u)}{w(u,u^\prime)} - \hspace{-8pt} \sum_{v^\prime \in N(v) \setminus u} \frac{w(v)}{w(v,v^\prime)}\right),
\end{equation}
where $N(v)$ denotes the neighborhood of a node $v$. 
When the graph $G$ is not weighted, i.e. when $w(v) = 1 = w(v,u)$ for all nodes $v$ and edges $(u,v)$, we get
\begin{equation}\label{eq:forman2}
F(u,v) = \left(2 - (\mathrm{deg}(v) - 1) - (\mathrm{deg}(u) - 1)\right) = 4 - (\mathrm{deg}(v) + \mathrm{deg}(u)).
\end{equation}

The general formula by Forman included the faces of any dimension, which can be done for graphs as well~\citep{weber2017coarse}. In~\Eqref{eq:forman} only $1$-dimensional faces (edges) are included. However, one can also include $2$-dimensional faces. This was considered in~\cite{samal2018comparative}, where $2$-dimensional faces on three nodes (triangles) were considered. In the case of an unweighted graph we then obtain
\begin{equation}\label{eq:def_forman_curvature_triangles}
	\hat F_G(u,v) = F(u,v) + 3\Delta(u,v) 
	= 4 - \mathrm{deg}(v) - \mathrm{deg}(u) + 3\Delta(u,v),
\end{equation}
where $\Delta(u,v)$ is the number of triangles that contain the edge $(u,v)$. 

\subsection{Approximate Ollivier curvature}

An explicit formula for Ollivier-Ricci curvature in graphs is obtained by~\citet{kelly2019self} under somewhat restrictive assumptions. In general, it could be used as an approximation.

For an edge $(u,v)$, let $\triangle_{uv}$, $\square_{uv}$ and $\pentagon_{uv}$ denote, respectively, the number of triangles, squares and pentagons in the graph that include the edge $(u,v)$. Moreover let $a \wedge b = \min\{a,b\}$, $a \vee b = \max\{a,b\}$ and $[a]^+ = a \vee 0$. Then we define:
\begin{multline}\label{eq:def_ollivier_ricci_approx}
		\kappa_G^\ast(u,v) = \frac{\triangle_{uv}}{\mathrm{deg}(u) \wedge \mathrm{deg}(v)} 
			- \left[1 - \frac{1 + \triangle_{uv} + \square_{uv}}{\mathrm{deg}(u) \vee \mathrm{deg}(v)} - \frac{1}{\mathrm{deg}(u) \wedge \mathrm{deg}(v)}\right]^+\\
	- \left(\frac{\triangle_{uv}}{\mathrm{deg}(u) \vee \mathrm{deg}(v)} 
			- \frac{\triangle_{uv}}{\mathrm{deg}(u) \wedge \mathrm{deg}(v)}\right) \vee \left(1 - \frac{1 + \triangle_{uv} + \square_{uv} + \pentagon_{uv}}
			{\mathrm{deg}(u) \vee \mathrm{deg}(v)}
			- \frac{1}{\mathrm{deg}(u) \wedge \mathrm{deg}(v)}\right).
\end{multline}

\citet{kelly2019self} shows that $\kappa_G^\ast(u,v) = \kappa_G(u,v)$ if and only if short cycles of length $3$, $4$ or $5$ that include the edge $(u,v)$ share no other edges. For example, if $w$ is a neighbor of both $u$ and $v$ (so $w$ is part of a $3$-cycle) then $w$ cannot be connected to any other neighbor of either $u$ or $v$, since that would mean that $w$ is part of a $4$-cycle that includes the edge $(u,v)$. Although this requirement is quite restrictive, one could still use~\Eqref{eq:def_ollivier_ricci_approx} as an approximation (or even a definition) of curvature in graphs.

\subsection{Heuristic sectional curvature}

A different notion of curvature used by \citep{gu2019learning} is based on the parallelogram law. For a parallelogram with nodes $a,b,c,d$ and sides $ab, ac, bd, cd$, the parallelogram law states that:
$2d(c,d)^2 + 2d(b,d)^2 = d(b,c)^2 + d(a,d)^2$.
Now imagine a point $m$ in the center of the line connecting $a$ and $d$, i.e. $d(a,m)=d(a,d)/2=d(m,d)$. Then we can derive that
\begin{equation}\label{eq:parallelogram_law}
	d(a,m)^2 + \frac{d(b,c)^2}{4} - \frac{d(a,b)^2 + d(a,c)^2}{2} = 0.
\end{equation}

The key intuition behind the definition in~\cite{gu2019learning} is that this holds in flat Euclidean space while in a positively or negative curved space the left hand side in~\Eqref{eq:parallelogram_law} should be positive or negative, respectively. 

For graphs, let $v$ be a node in $G$, $u,w$ neighbors of $v$ and $z$ any other node. Then we define
\begin{equation}
	\xi_G(v,u,w,z) = d(v,z)^2 + \frac{d(u,w)^2}{4} - \frac{d(v,u)^2 + d(v,w)^2}{2}.
\end{equation}
Note that this resembles the left hand side in~\Eqref{eq:parallelogram_law} with $a = v$, $b = u$, $c = w$ and $m = z$. Again, based on the parallelogram law, the intuition is that if these four vertices are supposed to be embedded in negative, flat or positive curved space then $\xi_G(v,u,w,z)$ should be negative, zero or positive, respectively. Now consider any node $v$ and two of its neighbors $u$ and $w$. Then we define the graph sectional curvature of a node $v$ and its neighbors $u,w$ as the average of $\xi_G(v,u,w,z)$,
\[
	\xi_G(v; u,w) = \frac{1}{|G|-1} \sum_{z \in G\setminus v} \frac{\xi_G(v,u,w,z)}{2d_G(v,z)}.
\] 
Here the normalization constant $2d_G(v,z)$ is included to yield the right scalings for trees and cycles.

The curvature of the graph is then defined as the average of $\xi_G(v; u,w)$. This is computed as $\xi_G = \frac{\xi_G(v^\ast,u^\ast,w^\ast,z^\ast)}{2d_G(v^\ast,z^\ast)}$, where $v^\ast, u^\ast, w^\ast$ and $z^\ast$ are selected uniformly at random from $G$, $N(v^\ast)$, $N(v^\ast)\setminus u^\ast$ and $G\setminus v^\ast$, respectively. Here $N(v)$ denotes the neighborhood of $v$.

\textbf{[To be rewritten and corrected: we have to swap $z$ and $v$, according to the original paper.]}

\subsection{Gromov hyperbolicity}

\textbf{[Define and discuss that it cannot be easily used to define curvature. Probably not in this section.]}

\section{Global graph curvature}

\subsection{Definition}

While there are many different notions of \textit{local} graph curvature, they cannot be easily used in practical application. In practice, data is usually embedded in a space of constant curvature, so a \textit{global} notion of curvature is needed. 
In this paper, we propose a practice-driven definition of global graph curvature and compute this curvature in some simple cases.

For a graph $G$ let $e(G)$ be the embedding of this graph into some metric space. Assume that we are given a loss function $L(e(G),G)$ for embedding task. For example, $L$ can be distortion. Now, let $e_{d,C}(G)$ be such embedding of a graph into a space of curvature $C$ and dimension $d$ that minimizes the loss function:
\[
e_{d,C}(G) = \argmin_{e(G) \in ...} L(e(G),G).
\]
We can define a $d$-dimensional curvature of $G$ in the following way:
\[
C_d(G) = \argmin_C L(e_{d,C}(G),G)\,.
\]
Note that $C_d(G)$ can be a set of points. 

\textbf{[At some point we should mention that we ignore precision problem. The only way I can see how we can take precision into account is to choose the maximum value among the set $C_d(G)$.]}

\subsection{Approximations}

\textbf{[Discuss that all local curvatures can potentially be used to define global one, just by averaging. Also, define our new curvature.]}

\subsection{Theoretical analysis of global curvature and its approximations}

\subsection{Star on $n+1$ nodes}

It is known that trees are negatively curved. We start our theoretical analysis with the simplest tree of depth 1 on $n+1$ nodes (one parent and $n$ children).

\paragraph{Ollivier curvature} 
Consider any tree graph $T$, let $v \sim u$ be two neighbors. Then Proposition 2 in~\citep{jost2014ollivier} states that
\begin{equation}\label{eq:ollivier_tree}
	\kappa_T(m_x,m_y) = -2\left(1 - \frac{1}{\mathrm{deg}(v)} - \frac{1}{\mathrm{deg}(u)}\right)^+, 
\end{equation}
where $t^+ = \max\{0,t\}$. In particular, if either $\mathrm{deg}(v) = 1$ or $\mathrm{deg}(u) = 1$, then $\kappa_T(u,v) = 0$.  As a result, for a star-graph $G$, we have $\kappa_G(u,v) = 0$ for all edges, so the average value is $\kappa_G = 0$.

\paragraph{Forman curvature}
If follows from \Eqref{eq:forman} and \Eqref{eq:forman2} that
$F_G = \hat F_G = 3-n$ (Forman curvature is the same for all edges).

\paragraph{Approximate Ollivier curvature} According to \Eqref{eq:def_ollivier_ricci_approx}, $\kappa_G^* = 0$.

\paragraph{Heuristic sectional curvature}

Sectional curvature is defined for a node and its two neighbors, in case of a star we can only take a central node $v$ and two neighboring one $u$ and $w$. In this case, for any other node $z$, we obtain $\xi_G(v;u,w;z) = -1$. Therefore, by averaging we obtain $\xi_G = -1$.

\paragraph{Distortion-based curvature} 
We prove the following theorem (the proof is in Appendix).

\begin{theorem}\label{thm:star_distortion}
For any $d$, we have $C_{G,d} = -\infty$.
\end{theorem}


\iffalse

%However, if $\mathrm{deg}(v) \ge 3$ for all nodes $v$ then $\kappa_G(v,u) < 0$. Thus trees are intrinsic examples of negatively curved graphs, according to Ollivier-Ricci curvature.






\paragraph{Threshold-based curvature}

We have $C_d(G) = (-\infty, C)$, where we can compute $C$ exactly for $d = 2$ and approximately for larger $d$. 

Note: for now I assume $b$ to be large enough to be in hyperbolic space only. But it is easy to see that the general statement about $(-\infty, C)$ is true always.

\begin{proof}

If $d = 2$, we want to have $n-1$ points to be at distance at most $1$ from $v$, but at distance $> 1$ from each other. It is easy to show that it is optimal to place the leaf nodes at distance exactly 1. Now $C$ is just the largest curvature which allows to have distance exactly 1 between closest leaves. Let us find $C$. We use hyperbolic low of cosines again and let $c = -\frac{1}{k^2}$, $\alpha = \frac{2\pi}{n-1}$:
%\[
%\cosh{\frac{1}{k}} = \cosh^2{\frac{1}{k}} - \sinh^2{\frac{1}{k}}\cos\alpha.
%\]
%\[
%\cosh{\frac{1}{k}} = 1 +  (1- \cos\alpha) \sinh^2{\frac{1}{k}}.
%\]
%\[
%\cosh{\frac{1}{k}} = 1 +  2\sin^2 \frac{\alpha}{2} \sinh^2{\frac{1}{k}}.
%\]
%\[
%\frac{1}{2\sin^2 \frac{\alpha}{2}} = \frac{\sinh^2{\frac{1}{k}}}{\cosh{\frac{1}{k}} - 1}.
%\]
%\[
%\frac{1}{2\sin \frac{\alpha}{2}} = \frac{\sinh{\frac{1}{k}}}{\sqrt{2}\sqrt{\cosh{\frac{1}{k}} - 1}}.
%\]
\[
\cosh\frac{1}{2k} = \frac{1}{2\sin \frac{\alpha}{2}}
\]
\[
k = \frac{1}{2\,\textrm{arccosh}\frac{1}{2\sin \frac{\alpha}{2}}}
\]
\[
C = - \left(2\,\textrm{arccosh}\frac{1}{2\sin \frac{\alpha}{2}}\right)^2
\]

Note that for $n = 6$ we have $C = 0$.

Also note that if $n$ is large, then $\sin \frac{\alpha}{2} = \sin\frac{\pi}{n-1} \sim \frac{\pi}{n-1}$. Then,  $\textrm{arccosh}\frac{1}{2\sin \frac{\alpha}{2}} \sim \textrm{arccosh}\frac{n-1}{2\pi} \sim \log n$, so we get $C \sim - 4 \log^2 n$. 

For $d > 2$ we can get lower and upper bounds for $C$. One bound we can obtain by covering the surface of a unit ball by the balls of radius $1/2$. The other bound can be obtained by using spherical packing. \textbf{[To be finished formally.]} 

\end{proof}


\subsection{Cycle of size $n$}
                     
\paragraph{Ollivier-Ricci curvature} $\kappa_G(u,v) = 0$ for all edges, so by averaging we obtain $0$.

\paragraph{Forman-Ricci curvature} $F(u,v) = 0 = F_2(u,v)$.

\paragraph{Ollivier approximation} $\kappa_G^* = 0$.

\paragraph{Distortion-based curvature} For any dimension $d$ and for $n \ge 4$, there is a unique curvature which gives zero distortion:
\[
C_d(G) = \left(\frac{2\pi}{n}\right)^2.
\]
This is easy to explain: if we consider three consequent vertices, then the middle one should lie on the geodetic between the other two. So, they all lie on a great circle (of length $n$).

\paragraph{Threshold-based curvature} 
Here, again, we have $C_d(G) = (-\infty, C)$ with some $C>0$. It is easy to prove that any curvature $c < \left(\frac{4\pi}{n}\right)^2$ is ok, but I have to think whether we can get some larger bound. 

\subsection{Complete graph on $n$ vertices}

\paragraph{Ollivier-Ricci curvature} 
        
$\kappa_G(u,v) = \frac{n-2}{n-1}$, so average is the same.

\paragraph{Forman-Ricci curvature} $F(u,v) = 6 - 2n$, $F_2(u,v) = n$.

\paragraph{Ollivier approximation} $\kappa_G^* = ...$. \textbf{[Easy, but did not check yet. We can see the difference with Ollivier with this example.]}

\paragraph{Distortion-based curvature} Assume that $d = n-2$, then it is easy to find $C_d(G)$. Note that we want to get $n-1$-simplex, which can be embedded into a $n-2$-dimensional spherical space, the radius is $R = a\sqrt{\frac{n-1}{2n}}$, and we want
\[
2 R \arcsin \frac{a}{2R} = 1.
\]
\[
2 a\sqrt{\frac{n-1}{2n}} \arcsin \sqrt{\frac{n}{2(n-1)}} = 1.
\]
\[
a  = \sqrt{\frac{2n}{(n-1)}}\frac{1}{2\arcsin \sqrt{\frac{n}{2(n-1)}}}.
\]
\[
R = \frac{1}{2\arcsin \sqrt{\frac{n}{2(n-1)}}}
\]

so we have
\[
C_{n-2}(G) = 4 \left(\arcsin \sqrt{\frac{n}{2(n-1)}}\right)^2\,.
\]

Also, it seems that for all $n$ and $d$ curvature $-\infty$ gives zero distortion since we can make a star from a clique using one Steiner node.

\paragraph{Threshold-based curvature} We can embed cliques to any space (all nodes to one point).

\subsection{Tree with branching factor $b$}

\paragraph{Distortion-based curvature} It is $-\infty$, similarly to stars.

\paragraph{Threshold-based curvature} This can be analyzed for $d = 2$ and we can get lower and upper bounds. One bound follows from the corresponding result for star on $b+2$ vertices (we need enough space to embed the children of one node). For the other bound, I have the precise combinatorial construction which gives $z\log{b}$ with some constant $z$ for which I have some analytical expression. I can write more details about this later, but the basic idea is that each level of a tree lies on a circle of some radius. This idea is well-known but I was not able to find rigorous result for our loss function.

\fi

%\begin{table}[t]
%\caption{Sample table title}
%\label{sample-table}
%\begin{center}
%\begin{tabular}{ll}
%\multicolumn{1}{c}{\bf PART}  %&\multicolumn{1}{c}{\bf DESCRIPTION}
%\\ \hline \\
%Dendrite         &Input terminal \\
%Axon             &Output terminal \\
%Soma             &Cell body (contains cell nucleus) \\
%\end{tabular}
%\end{center}
%\end{table}

%\subsubsection*{Author Contributions}

%\subsubsection*{Acknowledgments}

\bibliography{references}
\bibliographystyle{iclr2020_conference}

\appendix

\section{Distortion-based curvature for simple graphs}

\subsection{Proof of Theorem~\ref{thm:star_distortion}(star on $n+1$ node)}

\begin{proof}

The idea of the proof is the following: we take any curvature $C$ and prove a lower bound on distortion (for given dimension $d$). Then, we obtain an upper bound on distortion which tends to zero as $C \to -\infty$, which gives the claimed result.

Let us analyze the lower bound for distortion. Recall that distortion of a graph is average distortion over all pairs of nodes. Let $v$ be the central node and $v_1, \ldots, v_n$ be its neighbors. Then,
\begin{multline*}
D = \frac{1}{\binom{n}{2}} \left( \sum_{v_i} {|d(f(v),f(v_i)) - 1|} + \sum_{v_i \neq v_j} \frac{|d(f(v_i),f(v_j)) - 2|}{2} \right).
\end{multline*}



$v_1, v_2, v_3$ be any other three nodes. Then, it is sufficient to find the lower bound for the following function: 
\[
D(G) \ge 
 \frac{{n \choose 3}}{{n \choose 2}} \left( \sum_{1\le i < j\le 3}  \frac{|d(f(v_i),f(v_j)) - 2|}{2(n-3)} + \sum_{1\le i \le 3}  \frac{|d(f(v_i),f(v)) - 1|}{{n-2 \choose 2}} \right).
\]

So, basically, we need to esitmate a ``weighted'' distortion of such four nodes. One idea can be the following: if we consider three angles at the node $v$, then at least one of them is at most $2 \pi / 3$, so we can get a lower bound only for this triangle. So, we need a lower about for

\[
D' =  \frac{|d(f(v_1),f(v_2)) - 2|}{2(n-3)} + \frac{|d(f(v_1),f(v)) - 1| + |d(f(v_2),f(v)) - 1|}{{n-2 \choose 2}}.
\]

\textbf{[This seems to be a doable technical task: we want to embed a triangle with one angle $2\pi/3$ into a space of a fixed curvature to minimize such weighted distortion. I will return to this later.]}

Let us analyze upper bound for distortion now. To do this, we want to find some ``good'' embedding. Let us consider hyperbolic space with curvature $c$ and with $d = 2$. Let $v$ be the central node, then we spread all other points on a circle of some radius $1 + \varepsilon$. The angle between them is $2 \pi / (n-1)$. So, our loss consists of two terms, the first one is
\[
\frac{(n-1) \varepsilon}{{n \choose 2}} = \frac{2\varepsilon}{n}.
\]
Let $c = - \frac{1}{k^2}$. The second term is (assuming that $n$ is even, but odd is similar)
\[
\frac{(n-1)}{{n\choose 2}}\sum_{i = 1}^{(n-2)/2}  \left(2 - k\cdot \mathrm{arccosh}\left( \sinh^2\left(\frac{1 + \varepsilon}{k}\left(1 - \cos \frac{2 \pi l}{(n-1)}\right) \right) + 1 \right)\right).
\]

This can be further optimized, but let us find some rough bound on that. Take $\varepsilon = 0$, then ther first term is zero. Also all angles replace by the smallest ones (the assumption on odd or even is not longer needed). Then the second term is:

\[
\frac{{n-1 \choose 2}}{{n\choose 2}} \left(2 - k\cdot \mathrm{arccosh}\left( \sinh^2\left(\frac{1}{k}\left(1 - \cos \frac{2 \pi}{(n-1)}\right) \right) + 1 \right)\right).
\]
Now note that 
\[
\cdot \mathrm{arccosh}\left(\left(1 - \cos \frac{2 \pi}{(n-1)}\right) \sinh^2\left(\frac{1}{k} \right) + 1 \right).
\]
This term tends to zero (for fixed $n$) as $k \to 0$ ($c \to \infty$).

\textbf{[Optional ToDo here: 1) show the better bound 2) relation between $n$ and $c$ (which $c$ we need to get distortion less than $\varepsilon$)]}

\end{proof}


\section{Distortion of Random Geometric Graphs}

Consider the random graph $G_n^r$ constructed by choosing $n$ points uniformly in the square $[\sqrt{n}/2, \sqrt{n}/2]$ and connecting two points $u$ and $v$ if $d(u,v) \le r$. For the remainder of this section we will assume that $r \ge 244 \sqrt{\log(n)}$, which is slightly larger than the threshold value $r_c = \sqrt{\log(n)/\pi}$ at which point the graph $G_n^r$ becomes connected a.a.s. and furthermore that $r \le \sqrt{n}/2$. We will identify nodes $v \in V$ with points and let $(x_v,y_v)$ denote their corresponding coordinates.

Clearly, the structure of the graph $G_n^r$ is completely determined by the coordinates $\{(x_v,y_v)\}_{v \in V}$ of the chosen points. Hence, the most natural and optimal embedding for this graph should be in the $2$-dimensional Euclidean plane using these original coordinates. However, if we do this then the distortion actually diverges.

We use the following result from~\citep{diaz2016relation} concerning the stretch in $G_n^r$:

\begin{theorem}\label{thm:stretch_rgg}
Let $G_n^r$ be constructed as above. Then, for each pair of nodes $u,v \in V$ with $d(u, v) > r$, there exists sequences $a_n^-, a_n^+ \to 0$, depending on $r$, such that the following holds with probability $1 - o(n^{-5/2})$:
\begin{enumerate}
\item If $d(u,v) \ge \max\{12\log(n)^{3/2}/r, 21 r \log(n)\}$:
\[
	d_G(u,v) \ge \left\lfloor \frac{d(u,v)}{r}\left(1 + a_n^-\right)\right\rfloor
\]
\item if $r \ge 224 \sqrt{\log(n)}$:
\[
	d_G(u,v) \le \left\lceil \frac{d(u,v)}{r}\left(1 + a_n^+\right)\right\rceil
\]
\end{enumerate}
\end{theorem}

\begin{lemma}
Let $f : V \to \mathbb{R}^2$ denote the embedding for the graph $G_n^r$ that assigns to each node $v$ it original coordinates $(x_v,y_v)$ and let $D_n(r)$ denote the distortion associated with this embedding. Then
\[
	\mathbb{E}[D_n(r)] = \Omega\left(\sqrt{\log(n)}\right).
\]
\end{lemma}

\begin{proof}
Let $S_n$ denote the event for which the statements of Theorem~\ref{thm:stretch_rgg} holds for all pairs $(u,v)$. Then since the probability that it holds for one pair is $1 - o(n^{-5/2})$, it follows that $\mathbb{P}(S_n) = 1 - o(n^{-1/2})$.

On the event $S_n$, it follows from the second statement of Theorem~\ref{thm:stretch_rgg} that for all $u,v$ with $d(u,v) > r$
\[
	\left|\frac{d(u,v)}{d_G(u,v)} - 1\right| \ge \frac{d(u,v)}{d_G(u,v)} - 1
	\ge \frac{r(d_G(u,v) - 1)}{d_G(u,v)(1+a_n^+)} - 1.
\]
Therefore, on the event $S_n$, we have that
\begin{align*}
	D_n(r) &= \frac{1}{\binom{n}{2}} \sum_{u \ne v} \left|\frac{d(u,v)}{d_G(u,v)} - 1\right|\\
	&\ge \frac{1}{\binom{n}{2}} \sum_{u \ne v \atop d(u,v) > r} \left|\frac{d(u,v)}{d_G(u,v)} - 1\right|\\
	&\ge \frac{1}{\binom{n}{2}} \sum_{u \ne v \atop d(u,v) > r} \left(\frac{r(d_G(u,v) - 1)}{d_G(u,v)(1+a_n^+)} - 1\right)\\
	&\ge \left(\frac{r}{1 + a_n^+} - \frac{r}{d_G(u,v)(1+a_n^+)}\right) 
		\frac{|\{u \ne v \, :\, d(u,v) > r\}|}{\binom{n}{2}} - 1\\
	&\ge \left(\frac{r}{1 + a_n^+} - \frac{r}{n(1+a_n^+)}\right) 
			\frac{|\{u \ne v \, :\, d(u,v) > r\}|}{\binom{n}{2}} - 1\\
	&\ge 224 \sqrt{\log(n)} \frac{1 - \frac{1}{n}}{1 + a_n^+} \frac{|\{u \ne v \, :\, d(u,v) > r\}|}{\binom{n}{2}} - 1.
\end{align*}
Next we note that since $224 \pi \log(n) \le \pi r^2 \le \pi n/4$, we have that the expected number of nodes at least a distance $r$ away from a given node is at least $(n - \pi r^2) = \Omega(n)$ (center) and at most $n - \pi r^2/4 = O(n)$ (the corners). We conclude that
\[
	\mathbb{E}[|\{u \ne v \, :\, d(u,v) > r\}|] = \Theta\left(\binom{n}{2}\right).
\]

The result now follows since
\begin{align*}
	\mathbb{E}[D_n(r)] &\ge \mathbb{E}[D_n(r) 1_{S_n}] \\
	&\ge 224 \sqrt{\log(n)} \frac{1 - \frac{1}{n}}{1 + a_n^+} 
		\frac{\mathbb{E}[|\{u \ne v \, :\, d(u,v) > r\}|]}{\binom{n}{2}}
		= \Omega(\sqrt{\log(n)}).
\end{align*}
\end{proof}


\end{document}





\section{Default Notation}

In an attempt to encourage standardized notation, we have included the
notation file from the textbook, \textit{Deep Learning}
\cite{goodfellow2016deep} available at
\url{https://github.com/goodfeli/dlbook_notation/}.  Use of this style
is not required and can be disabled by commenting out
\texttt{math\_commands.tex}.


\centerline{\bf Numbers and Arrays}
\bgroup
\def\arraystretch{1.5}
\begin{tabular}{p{1in}p{3.25in}}
$\displaystyle a$ & A scalar (integer or real)\\
$\displaystyle \va$ & A vector\\
$\displaystyle \mA$ & A matrix\\
$\displaystyle \tA$ & A tensor\\
$\displaystyle \mI_n$ & Identity matrix with $n$ rows and $n$ columns\\
$\displaystyle \mI$ & Identity matrix with dimensionality implied by context\\
$\displaystyle \ve^{(i)}$ & Standard basis vector $[0,\dots,0,1,0,\dots,0]$ with a 1 at position $i$\\
$\displaystyle \text{diag}(\va)$ & A square, diagonal matrix with diagonal entries given by $\va$\\
$\displaystyle \ra$ & A scalar random variable\\
$\displaystyle \rva$ & A vector-valued random variable\\
$\displaystyle \rmA$ & A matrix-valued random variable\\
\end{tabular}
\egroup
\vspace{0.25cm}

\centerline{\bf Sets and Graphs}
\bgroup
\def\arraystretch{1.5}

\begin{tabular}{p{1.25in}p{3.25in}}
$\displaystyle \sA$ & A set\\
$\displaystyle \R$ & The set of real numbers \\
$\displaystyle \{0, 1\}$ & The set containing 0 and 1 \\
$\displaystyle \{0, 1, \dots, n \}$ & The set of all integers between $0$ and $n$\\
$\displaystyle [a, b]$ & The real interval including $a$ and $b$\\
$\displaystyle (a, b]$ & The real interval excluding $a$ but including $b$\\
$\displaystyle \sA \backslash \sB$ & Set subtraction, i.e., the set containing the elements of $\sA$ that are not in $\sB$\\
$\displaystyle \gG$ & A graph\\
$\displaystyle \parents_\gG(\ervx_i)$ & The parents of $\ervx_i$ in $\gG$
\end{tabular}
\vspace{0.25cm}


\centerline{\bf Indexing}
\bgroup
\def\arraystretch{1.5}

\begin{tabular}{p{1.25in}p{3.25in}}
$\displaystyle \eva_i$ & Element $i$ of vector $\va$, with indexing starting at 1 \\
$\displaystyle \eva_{-i}$ & All elements of vector $\va$ except for element $i$ \\
$\displaystyle \emA_{i,j}$ & Element $i, j$ of matrix $\mA$ \\
$\displaystyle \mA_{i, :}$ & Row $i$ of matrix $\mA$ \\
$\displaystyle \mA_{:, i}$ & Column $i$ of matrix $\mA$ \\
$\displaystyle \etA_{i, j, k}$ & Element $(i, j, k)$ of a 3-D tensor $\tA$\\
$\displaystyle \tA_{:, :, i}$ & 2-D slice of a 3-D tensor\\
$\displaystyle \erva_i$ & Element $i$ of the random vector $\rva$ \\
\end{tabular}
\egroup
\vspace{0.25cm}


\centerline{\bf Calculus}
\bgroup
\def\arraystretch{1.5}
\begin{tabular}{p{1.25in}p{3.25in}}
% NOTE: the [2ex] on the next line adds extra height to that row of the table.
% Without that command, the fraction on the first line is too tall and collides
% with the fraction on the second line.
$\displaystyle\frac{d y} {d x}$ & Derivative of $y$ with respect to $x$\\ [2ex]
$\displaystyle \frac{\partial y} {\partial x} $ & Partial derivative of $y$ with respect to $x$ \\
$\displaystyle \nabla_\vx y $ & Gradient of $y$ with respect to $\vx$ \\
$\displaystyle \nabla_\mX y $ & Matrix derivatives of $y$ with respect to $\mX$ \\
$\displaystyle \nabla_\tX y $ & Tensor containing derivatives of $y$ with respect to $\tX$ \\
$\displaystyle \frac{\partial f}{\partial \vx} $ & Jacobian matrix $\mJ \in \R^{m\times n}$ of $f: \R^n \rightarrow \R^m$\\
$\displaystyle \nabla_\vx^2 f(\vx)\text{ or }\mH( f)(\vx)$ & The Hessian matrix of $f$ at input point $\vx$\\
$\displaystyle \int f(\vx) d\vx $ & Definite integral over the entire domain of $\vx$ \\
$\displaystyle \int_\sS f(\vx) d\vx$ & Definite integral with respect to $\vx$ over the set $\sS$ \\
\end{tabular}
\egroup
\vspace{0.25cm}

\centerline{\bf Probability and Information Theory}
\bgroup
\def\arraystretch{1.5}
\begin{tabular}{p{1.25in}p{3.25in}}
$\displaystyle P(\ra)$ & A probability distribution over a discrete variable\\
$\displaystyle p(\ra)$ & A probability distribution over a continuous variable, or over
a variable whose type has not been specified\\
$\displaystyle \ra \sim P$ & Random variable $\ra$ has distribution $P$\\% so thing on left of \sim should always be a random variable, with name beginning with \r
$\displaystyle  \E_{\rx\sim P} [ f(x) ]\text{ or } \E f(x)$ & Expectation of $f(x)$ with respect to $P(\rx)$ \\
$\displaystyle \Var(f(x)) $ &  Variance of $f(x)$ under $P(\rx)$ \\
$\displaystyle \Cov(f(x),g(x)) $ & Covariance of $f(x)$ and $g(x)$ under $P(\rx)$\\
$\displaystyle H(\rx) $ & Shannon entropy of the random variable $\rx$\\
$\displaystyle \KL ( P \Vert Q ) $ & Kullback-Leibler divergence of P and Q \\
$\displaystyle \mathcal{N} ( \vx ; \vmu , \mSigma)$ & Gaussian distribution %
over $\vx$ with mean $\vmu$ and covariance $\mSigma$ \\
\end{tabular}
\egroup
\vspace{0.25cm}

\centerline{\bf Functions}
\bgroup
\def\arraystretch{1.5}
\begin{tabular}{p{1.25in}p{3.25in}}
$\displaystyle f: \sA \rightarrow \sB$ & The function $f$ with domain $\sA$ and range $\sB$\\
$\displaystyle f \circ g $ & Composition of the functions $f$ and $g$ \\
  $\displaystyle f(\vx ; \vtheta) $ & A function of $\vx$ parametrized by $\vtheta$.
  (Sometimes we write $f(\vx)$ and omit the argument $\vtheta$ to lighten notation) \\
$\displaystyle \log x$ & Natural logarithm of $x$ \\
$\displaystyle \sigma(x)$ & Logistic sigmoid, $\displaystyle \frac{1} {1 + \exp(-x)}$ \\
$\displaystyle \zeta(x)$ & Softplus, $\log(1 + \exp(x))$ \\
$\displaystyle || \vx ||_p $ & $\normlp$ norm of $\vx$ \\
$\displaystyle || \vx || $ & $\normltwo$ norm of $\vx$ \\
$\displaystyle x^+$ & Positive part of $x$, i.e., $\max(0,x)$\\
$\displaystyle \1_\mathrm{condition}$ & is 1 if the condition is true, 0 otherwise\\
\end{tabular}
\egroup
\vspace{0.25cm}


\paragraph{Circle}

Consider a circle on $n$ nodes and let $v \sim u$ be two neighbors. Then $W_1^G(m_u,m_v) = 1$ and hence $\kappa_G(u,v) = 0$.



\paragraph{Lattice}
Consider the $2$-dimensional lattice $\mathbb{Z}^2$ and let $v \sim u$ be two neighbors. Then the optimal transportation from $m_u$ to $m_v$ is given by a translation by the vector $y - x$ and hence $W_1^G(m_v,m_u) = d_G(u,v) = 1$, see also \cite[Example 5]{ollivier2009ricci}, and thus $\kappa_G(u,v) = 0$.

\paragraph{Complete graph}

Consider a complete graph on $n$ nodes. Then, for any two nodes $u$ and $v$ it follows from Example 1 in~\cite{jost2014ollivier} that $\kappa_G(m_v, m_u) = \frac{n-2}{n-1} \to 1$ as $n \to \infty$.

\paragraph{Erd\H{o}s-Renyi graph}

The following results from~\cite{lin2011ricci} characterize Ollivier-Ricci curvature in $G(n,p)$:

\begin{enumerate}
\item If $p \ge \log(n)^{1/3} n^{-1/3}$ then almost surely
\[
	\kappa_G(u,v) = p + O\left(\sqrt{\frac{\log(n)}{n p}}\right).
\]
\item If $2 \log(n)^{1/2} n^{-1/2} \le p < \log(n)^{1/3} n^{-1/3}$ then almost surely
\[
	\kappa_G(u,v) = O\left(\frac{\log(n)}{n p^2}\right).
\]
\item If $\log(n)^{1/2} n^{-2/3} \ll p \ll \log(n)^{-1/2}$ then almost surely
\[
	\kappa_G(u,v) = -1 + O\left(n p^2\right) + O\left(\frac{\log(n)}{n^2 p^3}\right).
\]
\item If $\log(n) n^{-1} \ll p \ll n^{-2/3}$ then almost surely
\[
	\kappa_G(u,v) = -2 + O\left(n^2 p^3\right) + O\left(\sqrt{\frac{\log(n)}{np}}\right)
\]
\end{enumerate}