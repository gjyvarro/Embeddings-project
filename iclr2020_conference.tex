\documentclass{article} % For LaTeX2e
\usepackage{iclr2020_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{wasysym}
\usepackage{amsthm}
\usepackage{graphicx}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{assumption}[definition]{Assumption}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{remark}[theorem]{Remark}


\title{Global graph curvature}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.  Funding acknowledgements go at the end of the paper.} \\
Department of Computer Science\\
Cranberry-Lemon University\\
Pittsburgh, PA 15213, USA \\
\texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
\And
Ji Q. Ren \& Yevgeny LeNet \\
Department of Computational Neuroscience \\
University of the Witwatersrand \\
Joburg, South Africa \\
\texttt{\{robot,net\}@wits.ac.za} \\
\AND
Coauthor \\
Affiliation \\
Address \\
\texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

%\newcommand{\fix}{\marginpar{FIX}}
%\newcommand{\new}{\marginpar{NEW}}
%\newcommand{\ph}[1]{\textcolor{blue}{#1}}
%\newcommand{\lt}[1]{\textcolor{red}{#1}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Recently, non-Euclidean spaces became popular for embedding structured data. However, determining suitable geometry and, in particular, curvature for a given dataset is still an open problem. 
In this paper, we define a notion of global graph curvature, specifically catered to the problem of embedding graphs, and analyze the problem of estimating this curvature using only graph-based characteristics (without actual embedding the graph).
We show that optimal curvature essentially depends on the dimensionality of the embedding space and loss function one aims to minimize via embedding. We review existing notions of local curvature (e.g., Ollivier-Ricci curvature) and analyze their properties theoretically and empirically. In particular, we show that such curvatures are often unable to properly estimate the global one. We also make an important observation: larger dimensionality usually leads to less negative optimal curvature. We propose a new estimator of global graph curvature which, in contrast to the existing ones, reflects this property as it is dimension-specific. We demonstrate that for a given dimension the new estimator is able to distinguish between ``hyperbolic'' and ``Euclidean'' networks. 

\end{abstract}

\section{Introduction}

Representation learning is an important tool for learning from structured data such as graphs or texts~\citep{grover2016node2vec,perozzi2014deepwalk,mikolov2013distributed}.
State-of-the-art algorithms typically use Euclidean space for embedding. 
Recently, however, it was found that hyperbolic spaces demonstrate superior performance for various tasks~\citep{nickel2018learning,sala2018representation}, while in some cases spherical spaces can be useful~\citep{liu2017sphereface}. A key characteristic classifying the above-mentioned spaces is curvature, which is negative for hyperbolic spaces, zero for Euclidean and positive for spherical spaces. These findings, therefore, show that certain graphs are better represented in spaces with non-zero curvature. While some methods simply fix curvature (e.g., -1 for hyperbolic space) and then find the optimal embedding of the graph in the corresponding space \citep{nickel2018learning}, others try to learn the right curvature and embedding simultaneously~\citep{gu2019learning}.

In this paper, we analyze the problem of determining a graph curvature suitable for embedding \textit{without} actual embedding the graph itself. The aim is to use some simple graph characteristics to understand whether it is reasonable to embed a graph into some non-Euclidean space and if yes, then which curvature to choose. 
Having such an estimator would save computational resources, since learning an embedding is computationally expensive and should be done for each space separately. 
More importantly, it can also save human resources by showing whether it is worth investing resources in the implementation of a more complex embedding algorithm.

We make an important first step in this direction by introducing a concept of \textit{global} graph curvature, which depends on both the dimension and loss function used for the embedding. We consider two loss functions: \emph{distortion}, which is widely used in embedding literature, and \emph{threshold-based} loss (see Section~\ref{sec:related_work}), which is more suitable for some practical applications. We demonstrate, both theoretically and empirically, that these loss functions lead to fundamentally different graph curvatures. 
We also compare several estimators of the global curvature, in particular, the ones based on well-known Ollivier-Ricci and Forman-Ricci \textit{local} graph curvatures. Our analysis shows that all these notions give curvatures that are far from the optimal curvature for embedding the graph (in particular, because they are dimension-independent). This raises the important question of how to properly estimate the best curvature for embedding graphs. 
We approach this problem by introducing a new curvature estimator which, in contrast to all existing ones, is dimension-specific. This estimator agrees with our theoretical results and is also able to distinguish between ``hyperbolic'' and ``Euclidean'' networks.

One important insight, which we gained based on both theoretical and empirical analysis, is the fact that the optimal curvature is usually smaller for smaller dimensions. In other words, if for some dimension the optimal curvature is highly negative, it usually becomes close to zero as dimensionality grows.

Finally, while in the current research we assume that the global graph curvature is constant, this concept can be extended in the future to cover, e.g., the product spaces from~\citep{gu2019learning}. We focused on the constant curvature since, first, such spaces are widely used in various applications~\citep{tifrea2018poincar} and, second, even for constant curvature we observed many non-trivial theoretical and empirical results.
These results and insights can eventually be extrapolated to analyze curvature in other, more complicated spaces. To the best of our knowledge, the problem of analyzing different notions of curvature for embedding graphs (even in simple spaces) was not considered before, so we hope that more results will follow.



\section{Background and related work}\label{sec:related_work}

\paragraph{Graph embeddings} 
For an unweighted graph $G = (V,E)$ equipped with the shortest path distance metric $d_G$, a graph embedding $f$ is a map $f: V \rightarrow U$, where $U$ is a metric space.
We refer to \cite{goyal2018graph} for a survey of several graph embedding methods and their applications. 

The goal of an embedding is to preserve some structural properties of a graph.
Depending on an application, different loss/quality functions are used to measure the quality of a graph embedding. The most well-known is \textit{distortion}:
\[
D(f) = \frac{1}{\binom{n}{2}} \sum_{u \neq v}  \frac{|d(f(u),f(v)) - d_G(u,v)|}{d_G(u,v)}.
\]

Distortion is a global metric, it takes into account all graph distances. However, in many practical applications, it may not be a good choice. For example, in recommendation tasks, we usually deal with a partially observed graph, so a huge graph distance between nodes in the observed part does not necessarily mean that the nodes are not connected in the full graph. Additionally, as discussed in Section~\ref{sec:S_n}, graph distances are hard to preserve: there are graphs on just 4 nodes that cannot be perfectly embedded in a space of any curvature and any dimension. 

Another measure, often used for embeddings, is Mean Average Precision (MAP), which, for a given node $v$, compares the distance-based ranking of other embedded nodes with the graph-neighborhood-based ranking. In contrast to distortion, MAP is scale-invariant, as it cares only about the order. Since changing curvature is equivalent to changing scale, MAP is insensitive to curvature.\footnote{In other words, it is sufficient to consider only the curvatures -1, 0, 1, corresponding to hyperbolic, Euclidean and spherical spaces. Moreover, by considering a small enough region in hyperbolic or spherical space we get geometry similar to the Euclidean one, so for MAP it is important to distinguish only between -1 and~1.} 
However, in practical applications like recommendation systems, ranking scores are usually complemented by minimum score thresholds to have personalized sizes of top recommended items.
This motivated us to consider another loss function, which is at the same time local and scale-dependent. 

Let us define the following class of \textit{threshold-based} loss functions. Given an embedding $f$ of a graph $G$, we (re)construct a graph $G'$ in the following way: $v$ and $u$ are connected in $G'$ iff $d(f(v),f(u))\le 1$. Then, any loss function which is based on the comparison of $G$ and $G'$ is called threshold-based. While all our theoretical results hold for \textit{any} threshold-based loss function (see Section~\ref{sec:global}), in our experiments we measure and compare several functions of this type including, e.g., zero-one loss for the edge classification problem (whether a given node pair is connected or not), see Appendix~\ref{sec:setup} for definitions and discussion. Such loss functions are natural in many applications (graph reconstruction, link prediction, recommendations).

\paragraph{Hyperbolic and Spherical Spaces}

For many years, Euclidean space was the primary choice for data embeddings~\citep{goyal2018graph}. However, it turned out that many observed datasets are well fitted into hyperbolic space~\citep{krioukov2010hyperbolic}. \citet{nickel2017poincare} have shown that such hyperbolic embeddings can improve state-of-the-art quality in several practical tasks, e.g., lexical entailment and link prediction. On the other hand, spherical spaces are also used for embeddings~\citep{liu2017sphereface}. \citet{gu2019learning} goes even further by suggesting mixed spaces: product manifolds combining multiple copies of spherical, hyperbolic, and Euclidean spaces. 

The main advantage of hyperbolic space is that it is ``larger'': the volume of a ball grows exponentially with its radius. Hence, such spaces are well suited for tree-like structures. On the other hand, spherical spaces are suitable for embedding cycles~\citep{gu2019learning}. Spherical and hyperbolic spaces are parametrized by \textit{curvature} $c$, which is positive for spherical space and negative for hyperbolic space. As $c \to 0$, geometry of both these spaces becomes similar to the Euclidean one. We discuss some geometrical properties of these different spaces in Appendix~\ref{sec:properties}.


\section{Local graph curvatures}

While in this paper we analyze the \textit{global} graph curvature, there are several \textit{local} ones proposed in the literature. Many of them are based on the notion of sectional curvature and Ricci curvature defined for Riemannian manifolds. Intuitively, Ricci curvature controls whether the distance between small spheres is smaller or larger than the distance between the centers of the spheres. Ricci curvature is positive if small spheres are closer than their centers are. We refer to~\citep{jost2009geometry,oneill1983semiriemannian} for more details on Ricci curvature.

\paragraph{Ollivier curvature}

Ollivier curvature translates the definition of Ricci curvature to graphs. Again, the idea is to compare the distance between two small balls with the distance between their centers. The distance between balls is defined by the well-known optimal transport distance (a.k.a. Wasserstein distance or earth-mover distance).
%, Monge-Kantorovich-Rubinstein distance).
Formally, for a graph $G$ we consider the shortest path metric on $G$, denoted by $d_G$, and let $W_1^G$ denote the Wasserstein metric with respect to the metric space $(G,d_G)$. Furthermore, for each node $v$ we let $m_v$ denote the uniform probability measure on the neighbors of $v$, i.e., $m_v(u) = \frac{1_{u \sim v}}{\mathrm{deg}(v)}$, where $\mathrm{deg}(v)$ denotes the degree of $v$. Then, the classic definition\footnote{Note that Ollivier curvature is defined in much more generality in terms of metrics and random walks, see~\citep{ollivier2009ricci}. Thus, different version on graphs can be considered. \Eqref{eq:def_classic_ollivier_graphs} corresponds to the classical choices of graph distance and random walk on the graph.} of Ollivier curvature between two neighboring nodes $v \sim u$ in $G$ is defined as
\begin{equation}\label{eq:def_classic_ollivier_graphs}
	\kappa_G(u,v) = 1 - W_1^G(m_v, m_u).
\end{equation}
We note that Ollivier curvature always belongs to the interval $[-2,1]$~\citep{jost2014ollivier}.

\paragraph{Forman curvature}

Forman curvature~\citep{sreejith2016forman} is based on the discretization of Ricci curvature proposed by~\citet{forman2003bochner}. It is defined for a general weighted graph $G$, with both node and edge weights. %$w(u,v)$. 
%\begin{equation}\label{eq:forman}
%	F_G(u,v) = w(u,v) \left(\frac{w(u) + w(v)}{w(u,v)} - \hspace{-8pt}\sum_{u^\prime \in N(u)\setminus v}  \frac{w(u)}{w(u,u^\prime)} - \hspace{-8pt} \sum_{v^\prime \in N(v) \setminus u} \frac{w(v)}{w(v,v^\prime)}\right),
%\end{equation}
When the graph $G$ is not weighted, 
the definition becomes:
\begin{equation}\label{eq:forman1}
F_G(u,v) 
= 4 - (\mathrm{deg}(v) + \mathrm{deg}(u)).
\end{equation}

Forman was interested in a general discretization of curvature for Riemannian manifolds and his formula includes faces of any dimension. Although this can be translated to graphs~\citep{weber2017coarse}, the formula becomes quite cumbersome. Therefore, in~\Eqref{eq:forman2} only $1$-dimensional faces (edges) are included. One can extend this expression by including higher dimensional faces. This was considered in~\citep{samal2018comparative}, where $2$-dimensional faces on three nodes (triangles) were included. In the case of an unweighted graph, we then obtain
\begin{equation}\label{eq:forman2}
	\hat F_G(u,v) = F(u,v) + 3\Delta_{uv} 
	= 4 - \mathrm{deg}(v) - \mathrm{deg}(u) + 3\Delta_{uv},
\end{equation}
where $\Delta_{uv}$ is the number of triangles that contain the edge $(u,v)$. 

Based on the definitions, both Forman curvatures, especially $F_G(u,v)$, are expected to often be highly negative. Indeed, this is what we observe in Sections~\ref{sec:theory} and~\ref{sec:experiments}.

\paragraph{Heuristic sectional curvature}

A different notion of curvature used by \citet{gu2019learning} is based on the following geometric fact. 
Let $abc$ be a geodesic triangle and let $m$ be the (geodesic) midpoint of $bc$. Then the value
%\begin{equation}\label{eq:parallelogram_law}
$	d(a,m)^2 + \frac{d(b,c)^2}{4} - \frac{d(a,b)^2 + d(a,c)^2}{2}$
%\end{equation}
is equal to zero in euclidean space, is positive in spherical space and negative in hyperbolic space.

For graphs, let $v$ be a node in $G$, $b,c$ neighbors of $v$ and $a$ any other node. Then, we define
\begin{equation}
\xi_G(v;b,c;a) = \frac{1}{2 d_G(a,v)} \left( d_G(a,v)^2 + \frac{d_G(b,c)^2}{4} - \frac{d_G(a,b)^2 + d_G(a,c)^2}{2} \right).
\end{equation}
This resembles
%~\Eqref{eq:parallelogram_law} 
the formula above
with $m = v$ and the normalization constant $2d_G(v,a)$ is included to yield the right scalings for trees and cycles.
To define the graph sectional curvature of a node $v$ and its neighbors $b,c$, we average $\xi_G(v;b,c;a)$ over all possible $a$: 
$\xi_G(v; b,c) = \frac{1}{|V|-3} \sum_{a \in G\setminus \{v,b,c\}} \xi_G(v;b,c;a)$.\footnote{We assume that $a$ does not coincide with $b$ or $c$, which do not affect the average much, but makes our results in Section~\ref{sec:theory} more succinct.}

\section{Global graph curvature}\label{sec:global}

The problem with these different notions of \textit{local} graph curvature is that they cannot be easily used in practical applications, where data is usually embedded in a space of \textit{constant} curvature.
Hence, a \textit{global} notion of curvature is needed. 
In this section, we propose a practice-driven definition of global graph curvature, discuss how to estimate this curvature based on local notions and compare all curvatures theoretically and empirically for several simple graphs. 

\subsection{Definition}\label{sec:global_curvature_definition}

For a graph $G$, let $f(G)$ be an embedding of this graph into a $d$-dimensional space of constant curvature $c$ (spherical, Euclidean or hyperbolic). 
Assume that we are given a loss function $L(f)$ for the embedding task (see Section~\ref{sec:related_work}).
Now, let $L_{opt}(G,d,c)$ be the optimal loss for given $d$ and $c$:
$L_{opt}(G,d,c) = \min_{f} L(f).$
Then, we define $d$-dimensional curvature of $G$ in the following way:
\begin{equation}\label{eq:global_curvature}
C_{d}^{L}(G) = \argmin_c L_{opt}(G,d,c)\,.
\end{equation}
Note that there may be several values of curvature $c$ delivering the minimum of $L_{opt}(G,d,c)$, in this case we say that  $C_{d}^{L}(G)$ consists of all such points.\footnote{Further we slightly abuse notation by writing that $C_{d}^{L}(G)$ is a real value if such $c$ is unique and a set of values otherwise.}

Below we analyze global curvatures based on distortion ($C_d^{dist}(G)$) and threshold-based ($C_d^{thr}(G)$) loss functions. 
In the latter case, in experiments, we measure zero-one loss, but our theoretical results apply to any threshold-based loss, since 
$L_{opt}(G,d,c)$ reaches its minimum on ``perfect'' embeddings, where we precisely reconstruct the graph $G$.

\subsection{Approximations}

Let us discuss how local graph curvatures can be used to estimate the global one. In all cases, the standard practice is to average edge or sectional curvature over the graph.

\paragraph{Ollivier curvature} $\kappa(G) = \frac{1}{|E|}\sum\limits_{u\sim v} \kappa_G(u,v)$.

\paragraph{Forman curvature} $F(G) = \frac{1}{|E|}\sum\limits_{u\sim v} F_G(u,v)$, $\hat F(G) = \frac{1}{|E|}\sum\limits_{u\sim v} \hat F_G(u,v)$.

\paragraph{Average sectional curvature} Let $P_3$ denote the number of paths of length 3 in $G$, then $\xi(G) = \frac{1}{P_3} \sum\limits_{v \in V }\sum\limits_{b<c: b,c\in N(v)} \xi_G(v;b,c)$.

It is important to note that all curvatures discussed above do \textit{not} depend on neither dimension $d$ nor loss function $L$. However, as we show below, global curvature defined in Section~\ref{sec:global_curvature_definition} significantly depends on them.

Let us also mention that there is a concept of Gromov's hyperbolicity~\citep{gromov1987hyperbolic}, which is sometimes used to decide whether it is reasonable to embed a graph to a hyperbolic space. However, first, this estimator cannot be easily converted to curvature, and, second, it does not say anything about the sphericity of data.

\subsection{Theoretical analysis of global curvature and its approximations}\label{sec:theory}

To better understand the performance of the proposed approximations of global graph curvature,
we shall consider several basic graphs and analyze their global curvature and approximations both theoretically and empirically. By studying these graphs we also hope to gain insights into how classic graph topologies influence the curvature of the space in which they can be properly embedded. For more complex graphs, e.g., for combinations of simple graphs studied in this section, it becomes extremely hard to prove rigorous theoretical results. However, the obtained intuitions work in such scenarios, as we illustrate and discuss in Appendix~\ref{sec:combinations}. The experimental setup for our empirical illustrations is described in Appendix~\ref{sec:setup}. 

\subsubsection{Star $S_n$}\label{sec:S_n}

\begin{figure}
    \centering
    \includegraphics[width = 0.49 \textwidth]{star_distortion.pdf}
    \includegraphics[width = 0.49 \textwidth]{star_zero_one.pdf}
    \caption{Star $S_n$}
    \label{fig:star}
\end{figure}

It is pointed out in numerous papers that trees are negatively curved. 
We analyze this theoretically and start with the simplest tree: one central node and $n$ leaves. We denote this graph by $S_n$ and assume that $n \ge 3$.

\paragraph{Ollivier curvature} 
Consider any tree graph $T$, let $v, u$ be two neighbors. Then Proposition 2 in~\citep{jost2014ollivier} states that
\begin{equation}\label{eq:ollivier_tree}
	\kappa_T(u,v) = -2\left(1 - \frac{1}{\mathrm{deg}(v)} - \frac{1}{\mathrm{deg}(u)}\right)^+, 
\end{equation}
where $t^+ = \max\{0,t\}$. In particular, if either $\mathrm{deg}(v) = 1$ or $\mathrm{deg}(u) = 1$, then $\kappa_T(u,v) = 0$.  As a result, for a star we have $\kappa_{S_n}(u,v) = 0$, so $\kappa(S_n) = 0$ and stars are \textit{not} negatively curved according to Ollivier curvature.

\paragraph{Forman curvature}
If follows from \Eqref{eq:forman1} and \Eqref{eq:forman2} that
$F(S_n) = \hat F(S_n) = 3-n$, so stars are highly negatively curved for large $n$ according to Forman curvature.

\paragraph{Average sectional curvature}

Heuristic sectional curvature is defined for a node and its two neighbors. In case of a star we can only take a central node $v$ and two neighboring ones $b$ and $c$. For any other node $a$ we obtain $\xi_{S_n}(v;b,c;a) = -1$. Therefore, by averaging we obtain $\xi(S_n) = -1$.

\paragraph{Distortion-based curvature} 
We prove the following theorem (the proof is in Appendix~\ref{app:star_distortion}).

\begin{theorem}\label{thm:star_distortion}
Assume that $d$ and $n$ are fixed.
If $c$ is bounded below by a constant, then $D(S_n)$ is also bounded below by a constant. 
If $c \to -\infty$, then the optimal distortion $D_{opt}(S_n) = \Theta\left(\frac{1}{\sqrt{-c}}\right)$.
Therefore, for any $S_n$ we have $C_d^{dist}(S_n) = -\infty$.
\end{theorem}

This result is illustrated in Figure~\ref{fig:star} (left): distortion decreases as curvature becomes smaller.
The intuition behind this result is the following: we cannot embed a star $S_3$ with zero distortion into a space of any constant curvature and any dimension, because in case of zero distortion the central node $v$ has to lie on the geodesics between all pairs of leaves, so all 4 nodes have to belong to one geodesics, which is impossible. Moreover, the same problem occurs if any graph $G$ contains $S_3$ as an induced subgraph.
On the other hand, if $c \to -\infty$, we can spread all leaves of $S_n$ uniformly on a circle of radius 1 around the central node and distortion of such construction will tend to zero since the distance between the pairs of leaves will tend to 2 (triangles become thinner). 
Further we will see that if we minimize a threshold-based loss, then any tree can be perfectly embedded with $d = 2$.

\paragraph{Threshold-based curvature} 
Here we have the following theorem (the proof is in Appendix~\ref{app:star_threshold}).

\begin{theorem}\label{thm:star_threshold}
$C_d^{thr}(S_n) = (-\infty, C)$ for some $C = C(n,d)$, which increases with $d$ and decreases with $n$. In particular, for $d = 2$, if $n < 6$, then $C = \left(\arccos \frac{\cos \frac{2\pi}{n}}{1 - \cos \frac{2\pi}{n}}\right)^2$; if $n = 6$, then $C = 0$; if $n > 6$, then $C = - \left(2\,\mathrm{arccosh}\frac{1}{2\sin \frac{\pi}{n}}\right)^2$.
\end{theorem}

The result is illustrated in Figure~\ref{fig:star} (right). Note that zero-one loss can be noisy, especially on small graphs, since it is threshold-based, i.e., discrete.

\subsubsection{Tree $T_b$ with branching factor $b$}

%\begin{figure}
%    \centering
%    \includegraphics[width = 0.49 \textwidth]{tree_3_6_distortion.pdf}
%    \includegraphics[width = 0.49 \textwidth]{tree_3_6_zero_one.pdf}
%    \caption{Tree $T_{3,6}$ with branching factor $3$ and depth $6$}
%    \label{fig:tree}
%\end{figure}

We consider a tree $T_b$, $b \ge 2$. For symmetry, assume that the first node has $b+1$ children, while all other nodes have a parent and $b$ children. Also, for Ollivier, Forman and threshold-based curvatures, we assume that the depth of $T_b$ is infinite. In other cases, our statements hold for any finite tree $T$.

\paragraph{Ollivier curvature} We can use \Eqref{eq:ollivier_tree} and get, for any edge,
$\kappa_{T_b}(u,v) = -2 \left(1 - \frac{1}{b+1} - \frac{1}{b+1} \right) = - 2 + \frac{4}{b+1}$. So, $\kappa(T_b) = -2 + \frac{4}{b+1}$, which is negative.

\paragraph{Forman curvature} 
It is easy to see that we have 
$F(T_b) = \hat F(T_b) = 4 - 2(b+1) = 2(1-b)$.

%\paragraph{Approximate Ollivier curvature} 
%$\kappa_G^\ast(u,v) = -2 + \frac{4}{b+1}$.

\paragraph{Average sectional curvature} In contrast to Ollivier and Forman curvatures, heuristic sectional curvature is global, i.e., it depends on the whole graph, which has to be finite. Note that for any tree, to compute sectional curvature, we average 0 and -1. As a result, for any tree $T$ we have $\xi(T) \in [-1,0]$~\citep{gu2019learning}.

\paragraph{Distortion-based curvature} 

On the one hand, our result for $S_n$ implies that if a graph contains $S_3$, then it cannot be embedded with zero distortion in any space. One the other hand, \citet{sarkar2011low} proves that if we scale all edges by a sufficiently large factor $\tau$, then the obtained tree can be embedded to the hyperbolic plane with distortion at most $1 + \varepsilon$ with arbitrary small $\varepsilon$. Note that multiplying graph edges by $\tau$ is equivalent to changing curvature from $1$ to $\tau^2$. As a result, \citet{sarkar2011low} proves that we can achieve an arbitrary small distortion if $c \to -\infty$.
Hence, $C_d^{dist}(T) = -\infty$ for any $T$.

\paragraph{Threshold-based curvature} 

We prove the following theorem (see Appendix~\ref{app:tree_threshold}).

\begin{theorem}\label{thm:tree_threshold}
$C_d^{thr}(T_b) = (-\infty, C)$ for some $C = C(b,d)$, which increases with $d$ and decreases with $b$. 
The following lower bound holds: $C(b,2) \ge \left( \frac{2\,\log b}{2 \, \mathrm{arccosh} \frac{\cosh 1}{\cosh 1/2} - 1} \right)^2$.
\end{theorem}

Actually, the bound above holds for any tree whose branching is bounded by $b$.
Interestingly, while it is often claimed that trees are intrinsically hyperbolic, to the best of our knowledge, we are the first to formally prove that trees can be perfectly embedded in a hyperbolic plane of some curvature.  

\subsubsection{Complete graph $K_n$}

\paragraph{Ollivier curvature}  
For any two nodes $u$ and $v$, it follows from Example 1 in~\citep{jost2014ollivier} that $\kappa_{K_n}(v, u) = \frac{n-2}{n-1}$. Thus, $\kappa(K_n) = \frac{n-2}{n-1}$ and it tends to 1 as $n \to \infty$.


\begin{figure}
    \centering
    \includegraphics[width = 0.8 \textwidth]{clique_distortion.pdf}
    \caption{Complete graph $K_n$}
    \label{fig:clique}
\end{figure}

\paragraph{Forman curvature} 
Simple computations yield:
$F(K_n) = 6 - 2n$, $\hat{F}(K_n) = n$, i.e., we get either highly positive or highly negative value.

\paragraph{Average sectional curvature} It is easy to compute that
$\xi(K_n) = \frac{1}{8}$.

\paragraph{Distortion-based curvature}

The following theorem analyzes $C_d^{dist}(K_n)$ if $d = n-2$ (see Appendix~\ref{app:clique_distortion} for the proof). 

\begin{theorem}\label{thm:clique_distortion}
$C_{n-2}^{dist}(K_n) = \bigg\{-\infty,  4 \left(\arcsin \sqrt{\frac{n}{2(n-1)}}\right)^2\bigg\}$.
\end{theorem}

This result is illustrated in Figure~\ref{fig:clique}. For $n = 4, d = 2$ we expect to see the minimum at about $3.65$, for $n = 12, d = 2$ we expect $2.76$, which is indeed the case. For other parameters, we do not have theoretical results, however we see sudden drops in positive curvature. Finally, as expected, distortion decreases as curvature becomes small. 

\paragraph{Threshold-based curvature} 
$C_d^{thr}(K_n) = \R$, since we can embed any complete graph perfectly by mapping all nodes to one point. 

\subsubsection{Cycle graph $C_n$} 

\begin{figure}
    \centering
    \includegraphics[width = 0.49 \textwidth]{cycle_distortion.pdf}
    \includegraphics[width = 0.49 \textwidth]{cycle_zero_one.pdf}
    \caption{Cycle graph $C_{n}$}
    \label{fig:cycle}
\end{figure}

We consider a cycle $C_n$ with $n \ge 4$.

\paragraph{Ollivier curvature}  
Let $v \sim u$ be two neighbors. Then it is easy to see that $W_1^G(m_u,m_v) = 1$ and hence $\kappa_G(u,v) = 0$. Thus, $\kappa(C_n) = 0$.

\paragraph{Forman curvature} 

Similarly, it is easy to see that $F(C_n) = \hat F(C_n) = 0$.

\paragraph{Average sectional curvature} If $n$ is even, then $\xi_{C_n}(v;b,c;a) = 0$ for all points except the one diametrically opposite to $v$ for which we have $\xi_{C_n}(v;b,c;a) = 1$. If $n$ is odd, then for two points we have $\xi_{C_n}(v;b,c;a) = \frac{n}{2(n-1)}$. As a result, $\xi(C_n) = \frac{1}{n-3}$ for even $n$ and $\xi(C_n) = \frac{n}{(n-1)(n-3)}$ for odd $n$.

\paragraph{Distortion-based curvature} 

%\begin{proposition}\label{prop:cycle_distortion}
%$C_d^{dist}(C_n) = \left(\frac{2\pi}{n}\right)^2$.
%\end{proposition}
Here we have that
%\begin{equation}\label{eq:cycle_distortion}
$	C_d^{dist}(C_n) = \left(\frac{2\pi}{n}\right)^2.$
%\end{equation}
Indeed, if we consider any three consequent nodes, then the middle one should lie on the geodetic between the other two. So, they all lie on a great circle (of length $n$) from which the result follows.
In particular, $C_d^{dist}(C_5) \approx 1.58$, $C_d^{dist}(C_{100}) \approx 0.004$, which is illustrated in Figure~\ref{fig:cycle} (left).

\paragraph{Threshold-based curvature} 
It is easy to see that $C_d^{thr}(C_n) = (-\infty, C)$ with some $C>0$, which decreases with $n$ and increases with $d$ (see Figure~\ref{fig:cycle}, right). A simple lower bound for $C$ is $C \ge \left(\frac{4\pi}{n}\right)^2$, since for such curvature we can embed all nodes into a great circle with distances 1/2 between the closest ones. 


\subsubsection{Complete bipartite graph $K_{l,m}$}

\begin{figure}
    \centering
    \includegraphics[width = 0.49 \textwidth]{bipartite_distortion.pdf}
    \includegraphics[width = 0.49 \textwidth]{bipartite_zero_one.pdf}
    \caption{Complete bipartite graphs $K_{n,n}$}
    \label{fig:bipartite}
\end{figure}


W.l.o.g. we assume that $l \ge m \ge 2$ (the remaining cases are stars and are already considered). 

\paragraph{Ollivier curvature}  
We prove the following lemma (the proof is in Appendix~\ref{app:bipartite_ollivier}).

\begin{lemma}\label{lem:bipartite_ollivier}
$\kappa(K_{l,m}) = 0$.
\end{lemma}

\paragraph{Forman curvature} 

$F(K_{l,m}) = \hat F(K_{l,m}) = 4 - l - m$. 

%\paragraph{Approximate Ollivier curvature} 
%In is easy to derive from \Eqref{eq:def_ollivier_ricci_approx} that $\kappa^*(K_{l,m}) = 0$. \textcolor{red}{[Which is probably significantly different from true Ollivier curvature.]}

\paragraph{Average sectional curvature} 
The following lemma holds (see Appendix~\ref{app:bipartite_heuristic}).

\begin{lemma}\label{lem:bipartite_heuristic}
$
\xi({K_{l,m}}) = \frac{-(l - m)^2 + m + l - 2}{(m+l-2)(l+m-3)}.
$
In particular, if $m = l$ we get $\xi({K_{l,m}}) = \frac{1}{2m - 3}$. 
\end{lemma}
This lemma implies that for balanced complete bipartite graphs $\xi({K_{l,m}})$ is positive, but tends to zero as the graph grows.



\paragraph{Distortion-based curvature} 

Figure~\ref{fig:bipartite} (left) shows the empirical behavior of distortion depending on curvature. We observe that for all graphs optimal curvature (for the considered range) is between 2 and 2.5, where distortion has a drop. 
The following proposition gives an intuition about why one could expect this. 

\begin{proposition}\label{prop:bipartite_distortion}
For any $d$, $C_d^{dist}(K_{2,2}) = \left(\frac{\pi}{2}\right)^2 \approx 2.47$ and $K_{2,2}$ is the only complete bipartite graph (with at least two nodes in each part) for which zero distortion is achievable.
\end{proposition}

Indeed, the result for $K_{2,2}$ follows from the corresponding result on cycle $C_4$. Moreover, if for $K_{l,m}$ we have $l \ge 3$ and $m \ge 2$, then for any two nodes in the part of size $l$ there are at least 2 different geodesics of length 2 between them. Therefore, all such pairs lie at opposite poles of the hypersphere, which is impossible since $l \ge 3$.

%We also noticed in our experiments that very negative curvature can lead to even smaller distortion for bipartite graphs (see Table~\ref{tab:compare_distortion}).

\paragraph{Threshold-based curvature} 
 
Threshold-based embeddings to Euclidean spaces are well studied. In particular, it is know that for any graph $G(V,E)$ a perfect threshold embedding  exists for some $d \le |V|$~\citep{maehara1984space}. 
However, undirected bipartite graphs are hard to embed in Euclidean space: the bound $d = O(n)$ is known~\citep{maehara1984space}. We empirically observe that they are similarly hard to embed to both spherical and hyperbolic spaces, as shown in Figure~\ref{fig:bipartite} (right). Empirically, these graphs are insensitive to curvature (given that it is not too large).
 
\subsection{Volume-based approximation}

Let us propose a new heuristic curvature estimator which is motivated by the intuition obtained in the previous section as well as by extensive literature arguing that the most important feature of a hyperbolic space is the fact that it has ``more volume'' in the neighborhood.
Therefore, the natural idea is to estimate the volume we need for embedding nodes that are at distance at most $k$ from a given one and then compute the curvature needed to get such volume. 
We compare the number of nodes at distance exactly $k$ from a node (discounted using the number of edges between them) with the area of a hypersphere of radius $k$ in a space of curvature $c$. The detailed description of the procedure is given in Appendix~\ref{sec:new_curvature}. 
%As a result, we obtain an approximation of the curvature suitable for embedding.  Then, we take the minimum among this value and zero as the curvature. Based on the theoretical results, we say that a spherical space is expected to be not useful for threshold-based losses, so it is not worth the effort to embed there. On the other hand, when the estimated bound is negative, we take the largest value to avoid possible precision problems~\cite{sala2018representation}.
A nice feature of the proposed estimator is that it depends on dimension. Moreover, this dependence is natural: when $d$ becomes larger, curvature becomes smaller in absolute value. As we show in the experiments on various datasets, a similar property is observed for the optimal curvature.
Also, it turns out that for a given dimension the new estimator is able to distinguish well between negatively curved and neutral networks.
%Also, we show in the experiments that this estimator is conservative, i.e., in many cases it predicts curvature 0, which agrees with experiments. 
However, despite the potential of this new estimator, the problem of proper global curvature estimation is still wide open.
 
\section{Experiments}\label{sec:experiments}

\begin{table}[t]
\caption{Compare curvature estimators, distortion}
\label{tab:compare_distortion}
\begin{center}
\begin{tabular}{lcccccc|cc}
&
\multicolumn{2}{c}{Ollivier} &
\multicolumn{2}{c}{Forman} &
\multicolumn{2}{c}{Avg. sectional} &
\multicolumn{2}{|c}{Grad. descent} \\
%\multicolumn{2}{c}{Grid search} \\
Dataset/dim & c & loss  & c & loss  & c & loss  & c & loss \\% & c & loss \\
 \hline \\
%2.4 & 0.250 \\
%Karate / 2 &
%0.01 & \textbf{0.197} & 
%-6.35 & 0.248 &
%0.30 & 0.206 &
%-1.46 & \textbf{0.173} \\
%-1.8 & 0.160 \\
%Karate / 10 &
%0.01 & \textbf{0.105} & 
%-6.35 & 0.157 &
%0.30 & 0.126 &
%0.0 & 0.105 \\
%-0.4 & 0.100 \\
Conflict / 2 &
-0.2 &	\textbf{0.229} &
-16.5 &	0.261 &
0.25 & 	0.269 &
0.0 & \textbf{0.229} \\
Conflict / 10 &
-0.2 & \textbf{0.078} &
-16.5 &	0.243 &
0.25 &	0.173 &
0.0 & 0.085  \\
Chicago / 2 &
-0.19 & 0.225 & 
-8.37 & 0.224 & 
-0.6 & \textbf{0.178} & 
0.0 & 0.277 \\
Chicago / 10 &
-0.19 & 0.045 & 
-8.37 & 0.216 & 
-0.6 & \textbf{0.024} & 
0.0 & 0.080  \\
CSPhDs / 2 &
-0.28 &	\textbf{0.152} &
-7.92 &	0.420 &
-0.26 & 0.172 &
0.0 & 0.209 \\
CSPhDs / 10 &
-0.28 &	\textbf{0.085} &
-7.92 &	0.412 &
-0.26 & 0.208 &
0.0 & \textbf{0.056} \\
Euroroad / 2 &
-0.36 &	\textbf{0.267} &
-1.95 &	0.452&
0.027 &	0.384&
0.0 & \textbf{0.147}\\
Euroroad / 10 &
-0.36 &	\textbf{0.264} &
-1.95 &	0.445 &
0.03 &	0.370 &
0.0 & \textbf{0.052}  \\
EuroSiS / 2 &
-0.17 & 0.247 & 
-29.1 & \textbf{0.197} & 
0.27 & 0.298 & 
0.0 & 0.263 \\
EuroSiS / 10 &
-0.17 & \textbf{0.087} & 
-29.1 & 0.187 & 
0.26 & 0.202 & 
0.0 & 0.096 \\
Power / 2 &
-0.35 & \textbf{0.287} &
-2.85 & 0.368 &
0.02 & 0.363 &
-1.88 & 0.372  \\
Power / 10 &
-0.35 & \textbf{0.278} &
 -2.85 &0.361 &
 0.02 & 0.331 &
-1.0 & 0.400  \\
Facebook / 2 &
0.308 & 0.290 & 
-44.7 & 0.327 & 
0.153 & 0.240 & 
0.0 & 0.238  \\
Facebook / 10 &
0.308 & 0.210 &
-44.7 & 0.323 & 
0.153 & 0.084 & 
0.0 & 0.072 \\
\hline \\
\end{tabular}
\end{center}
\end{table}

In this section, we compare all discussed curvature estimators empirically. Our experiments are based on a publicly available implementation of graph embedding by~\citet{gu2019learning}.\footnote{\url{https://github.com/HazyResearch/hyperbolics}} The main advantage of this algorithm is that it works for any curvature: negative, positive or zero. We modified the implementation for our task: in particular, we added a regime responsible for minimizing threshold-based loss functions. The detailed description of our experimental setup is given in Appendix~\ref{sec:setup}, the datasets and their properties are listed in Appendix~\ref{sec:datasets}.

The results of the comparison for distortion and zero-one loss are presented in Tables~\ref{tab:compare_distortion} and~\ref{tab:compare_zero_one} (similar results on synthetic graphs are in Appendix~\ref{sec:compare_synth}), respectively.\footnote{Dimension 10 is not included for zero-one loss if for all curvatures a perfect embedding is possible.} We also added a method based on the gradient descent, proposed by~\citet{gu2019learning}. This method is used here only as an illustration, since our aim is to compare estimators which do not do actual embedding.
Also, the gradient descent algorithm uses the best space based on the value of the loss function, while all other curvature estimators do not use such information.
We marked in bold both the global winner and the winner among all curvatures except the gradient descent one. 

Table~\ref{tab:compare_distortion} analyzes distortion, here we compare Ollivier, Forman and average sectional curvatures and see that all of them have unstable performance. Also, even gradient descent method is not always the best (in both tables). In Table~\ref{tab:compare_zero_one} we also add volume-based estimator, which is designed for zero-one loss. 
One can see that it is often the best one and one of the reasons is that it often predicts zero curvature. In Appendix~\ref{sec:figures}, we present additional figures that illustrate the fundamental difference between distortion and zero-one loss: zero-one loss is often optimal in a space of zero curvature.

\begin{table}[t]
\caption{Compare curvature estimators, zero-one loss}
\label{tab:compare_zero_one}
\begin{center}
\begin{tabular}{lcccccccc|cc}
&
\multicolumn{2}{c}{Ollivier} &
\multicolumn{2}{c}{Forman} &
\multicolumn{2}{c}{Avg. sectional} &
\multicolumn{2}{c}{Volume} &
\multicolumn{2}{|c}{Grad. descent} 
%& \multicolumn{2}{c}{Grid search} 
\\
Dataset & c & loss  & c & loss  & c & loss & c & loss & c & loss  
\\
\hline \\
%Karate / 2 &
%0.01 & 0.135 & 
%-6.35 & 0.137 &
%0.30 & \textbf{0.123} &
%&&
%-19.1 & \textbf{0.116} 
%&-1.5 & 0.102
%\\
Conflict / 2 &
-0.2 & \textbf{0.032} &
-16.6 &\textbf{0.032} &
0.25 & 0.060 &
-2.21 & \textbf{0.032} &
0.0 & \textbf{0.032} \\
Conflict / 10 &
-0.2 & 0.001 &
-16.6 &	0.025 &
0.25 & $10^{-4}$ &
0.0 & \textbf{0.0} &
0.0 & \textbf{0.0}  \\
Chicago / 2 &
-0.19 & 0.004 & 
-8.37 & 0.005 & 
-0.6 & 0.004 & 
0.0 & \textbf{0.002} &
0.0 & \textbf{0.002}  \\
CSPhDs / 2 &
-0.28 &	\textbf{0.002} &
-7.92 &	0.004 &
-0.26 & \textbf{0.002} &
0.0 & \textbf{0.002} &
0.0 & \textbf{0.002}  \\
Euroroad / 2 &
-0.36 &	\textbf{0.002} &
-1.95 &	0.004 &
0.027 &	0.005 &
0.0 & \textbf{0.002} &
0.0 & \textbf{0.002} \\
EuroSiS / 2 &
-0.17 & \textbf{0.009} & 
-29.1 & 0.017 & 
0.26 & 0.058 & 
-3.79 & 0.016 &
0.0 & 0.010  \\
EuroSiS / 10 &
-0.17 & \textbf{0.003} & 
-29.1 & 0.012 & 
0.26 & \textbf{0.003} & 
0.0 & 0.007 &
0.0 & 0.007\\
Power / 2 &
-0.35 & \textbf{0.002} &
 -2.85 & 0.006 &
 0.02 & 0.006 &
0.0 & \textbf{0.002} &
0.0 & \textbf{0.002}  \\
Facebook / 2 &
0.308 & 0.058 & 
-44.7 & 0.059 & 
0.15 & \textbf{0.033} & 
-11.2 & 0.040 &
0.0 & \textbf{0.019} \\
Facebook / 10 &
0.308 & \textbf{0.007} & 
-44.7 & 0.050 & 
0.15 & \textbf{0.007} & 
0.0 & 0.013 &
0.0 & 0.013 \\
\end{tabular}
\end{center}
\end{table}

\section{Conclusion}

In this paper, we introduced a concept of global graph curvature motivated by the task of embedding graphs. The global graph curvature depends on the loss function and dimension of a space. To get an intuition about how this curvature behaves, we theoretically and empirically analyzed it for several simple graphs. We compared the global graph curvature and several approximations based on well-known local graph curvatures. We also demonstrated that the choice of loss function fundamentally affects the global curvature. We noticed that for zero-one loss it is usually optimal to choose zero curvature (Euclidean space) and proposed a simple estimator which decides whether it is reasonable to embed into a negatively curved space. Our work shows that the problem of finding the rights space for graph embedding is interesting and non-trivial and we hope our results will encourage further research on global graph curvature.
%and analyzed estimating of this curvature using only graph-based characteristics. 
%\subsubsection*{Author Contributions}

%\subsubsection*{Acknowledgments}

\bibliography{references}
\bibliographystyle{iclr2020_conference}

\appendix

\section{Geometrical properties of spaces of constant curvature}\label{sec:properties}

In this section, we recall some useful equalities which will be used throughout the proofs.

In all proofs, we use notation $R$, where $R = \frac{1}{\sqrt{c}}$ in spherical space (corresponds to the radius of a sphere) and in the hyperbolic case $(c < 0)$ $R = \frac{1}{\sqrt{-c}}$ can be considered as a scaling factor compared to the space of curvature~1.

\paragraph{Law of cosines}

Let us consider a triangle with angles $A, B, C$ and the lengths of opposite sides $a, b, c$, respectively.

In Euclidean, space we have:
\[
c^2 = a^2 + b^2 - 2 \, a\, b \cos C\,.
\]
In spherical space, the first law of cosines is:
\[
\cos \frac{c}{R} =\cos \frac{a}{R} \cos \frac{b}{R} + \sin \frac{a}{R}\sin \frac{b}{R}\cos C \,,
\]
and the second law of cosines is:
\[
\cos C=-\cos A\cos B+\sin A\sin B\cos \frac{c}{R}\,.
\]

In hyperbolic space, we have 
\[
\cosh {\frac {c}{R}}= \cosh {\frac {a}{R}}\cosh {\frac {b}{R}}-\sinh {\frac {a}{R}}\sinh {\frac {b}{R}}\cos C \,.
\]

\textbf{Equilateral triangle}

The following equalities follow from the corresponding laws of cosines, assuming that all sides (and angles) are equal.

For hyperbolic space:
\begin{equation}\label{eq:hyp_eq}
\cosh\frac{a}{2R} = \frac{1}{2\sin \frac{A}{2}}\,.
\end{equation}

For spherical space:
\begin{equation}\label{eq:sph_eq}
\cos{\frac{a}{R}} = \frac{\cos A}{1 - \cos A}.
\end{equation}

\paragraph{Area and volume of hypersphere}

Let $S_d(r)$ and $V_d(r)$ denote area of a hypersphere and volume of a ball of radius $r$ in $d$-dimensional space. 

In euclidean space,
\[
S_d(r)= d \, C_{d} \, r^{d-1},
\]
\[
V_d(r)= C_{d} \, r^{d},
\]
where
\[
C_{d}={\frac  {\pi ^{{d/2}}}{\Gamma ({d \over 2}+1)}}.
\]

In spherical space, sphere of radius $r$ is isometric to Euclidean sphere of radius $R \sin \frac{r}{R}$. Therefore, the area is 
\[
S_d(r)= d \, C_{d} \, \left( R \sin \frac{r}{R} \right)^{d-1},
\]
\[
V_d(r)= d \, C_{d} \, R^d \int_{0}^{r} \left( \sin \frac{x}{R} \right)^{d-1} d x .
\]

Similarly, in hyperbolic space,
\[
S_d(r)= d \, C_{d} \, \left( R \sinh \frac{r}{R} \right)^{d-1},
\]
\[
V_d(r)= d \, C_{d} \, R^d \int_{0}^{r} \left( \sinh \frac{x}{R} \right)^{d-1} d x .
\]


\section{Curvature of simple graphs}

\subsection{Proof of Lemma~\ref{lem:bipartite_ollivier} (Ollivier curvature for bipartite graphs)}\label{app:bipartite_ollivier}

Let use denote the node sets in $K_{l,m}$ by $U := \{u_1, \dots, u_l\}$ and $V := \{v_1, \dots, v_m\}$. We will prove that for any edge $(u, v)$, $W_1^{K_{l,m}}(m_{u},m_{v}) = 1$, which then implies that $\kappa(K_{l,m}) = 0$. For this we use the dual representation for the Wasserstein distance, see~\cite{ollivier2009ricci} for more details. This states that on the one hand
\[
	W^{G}_1(m_u,m_v) = \inf_{\rho} \sum_{v^\prime \sim v} \sum_{u^\prime \sim u} d_G(v^\prime, u^\prime) \rho(v^\prime, u^\prime),
\]
where the infimum is taken over all joint probability measures on the product of the neighborhoods of $v$ and $u$, while on the other hand
\[
	W^{G}_1(m_u,m_v) = \sup_{f} \left( \frac{1}{\mathrm{deg}(v) }\sum_{v^\prime \sim v} f(v^\prime) - \frac{1}{\mathrm{deg}(u) }\sum_{u^\prime \sim u} f(u^\prime)\right),
\]
where the supremum is taken over all $1$-Lipschitz functions, i.e $|f(u) - f(v)| \le d_G(u,v)$.

Note that for any $u \in U, v \in V$ the joint neighborhood is $V \times U$. First we establish an upper bound by considering the product joint probability density on $V \times U$
\[
	\rho(x,y) =	\frac{1}{m l}.
\]
It then follows that
\[
	W_1^{K_{l,m}}(m_{u},m_{v}) \le \sum_{i = 1}^m \sum_{j = 1}^l d_G(v_i, u_j) \rho(v_i, u_j)
	= 1.
\]
For the lower bound we define the function
\[
	f(z) = \begin{cases}
		2 &\mbox{if } z \in V,\\
		1 &\mbox{if } z \in U.
	\end{cases}
\]
Observe that if $u \in U$ and $v \in V$ then $|f(u) - f(z)| = 1 = d_G(u,v)$. On the other hand, if $u, u^\prime \in U$ then $|f(u) - f(u^\prime)| = 0 \le 2 = d_G(u,u^\prime)$ and similar for $v, v^\prime \in V$. Thus we conclude that $f$ is 1-Lipschitz. It now follows that
\[
	W_1^{K_{l,m}}(m_u,m_v) \ge \frac{1}{m} \sum_{i = 1}^m f(v_i) - \frac{1}{l} \sum_{j = 1}^l f(u_j) = 1,
\]
which completes the proof.

\subsection{Proof of Lemma~\ref{lem:bipartite_heuristic} (average sectional curvature for bipartite graphs)}\label{app:bipartite_heuristic}

If $v$ and $a$ are in the same part of the bipartite graph, then $\xi_{K_{l,m}}(v;b,c;a) = 1$, otherwise $\xi_{K_{l,m}}(v;b,c;a) = -1$. Therefore, if $a$ belongs to the part of size $l$, sectional curvature is
$\xi_{K_{l,m}}(v;b,c) = \frac{l-m+1}{l+m-3}$, otherwise it is $\xi_{K_{l,m}}(v;b,c) = \frac{m-l+1}{l+m-3}$. As a result, by averaging over all triplets, we get

\[
\xi({K_{l,m}}) = \frac{1}{l \binom{m}{2} + m \binom{l}{2}} \left(l \binom{m}{2} \frac{l-m+1}{l+m-3} + m \binom{l}{2} \frac{m-l+1}{l+m-3}\right) = \frac{-(l - m)^2 + m + l - 2}{(m+l-2)(l+m-3)}.
\]

\subsection{Proof of Theorem~\ref{thm:star_distortion} (distortion-based curvature for stars)}\label{app:star_distortion}

The general idea is the following: we take any curvature $c$ and prove a lower bound on distortion (for any given dimension $d$). Then, we obtain an upper bound on optimal distortion which tends to zero as $c \to -\infty$, which gives the claimed result $C_d^{dist}(S_n) = -\infty$.

First, let us analyze the lower bound on distortion. Recall that distortion of a graph is the average distortion over all pairs of nodes. Let $v$ be the central node and $v_1, \ldots, v_n$ ($n \ge 3$) be its neighbors. Then, for any embedding $f$, we have
\begin{multline*}
D(S_n) = \frac{1}{\binom{n+1}{2}} \left( \sum_{v_i} {|d(f(v),f(v_i)) - 1|} + \sum_{v_i \neq v_j} \frac{|d(f(v_i),f(v_j)) - 2|}{2} \right) \\
= \frac{1}{\binom{n+1}{2}} \sum_{1 \le i_1 < i_2 < i_3 \le n} \left(
\sum_{1\le j \le 3}  \frac{|d(f(v_{i_j}),f(v)) - 1|}{{n-1 \choose 2}} +
\sum_{1\le j < k\le 3}  \frac{|d(f(v_{i_j}),f(v_{i_k})) - 2|}{2(n-2)}   \right).
\end{multline*}

Let $D_{min}$ be the minimum value of the following weighted distortion of a star with 3 leaves:
\[
D_{min} = \min_{f} \sum_{1\le j \le 3}  \frac{|d(f(v_j),f(v)) - 1|}{(n-1)/4} +
\sum_{1\le j < k\le 3}  |d(f(v_j),f(v_k)) - 2|,
\]
then 
\begin{equation}\label{eq:D_min}
D(S_n) \ge \frac{{n\choose 3}}{2(n-2){n+1 \choose 2}} D_{min} = \frac{ (n-1)D_{min}}{6(n+1)}\,.
\end{equation}
Hence, it remains to find a lower bound on $D_{min}$, i.e., a lower bound for a weighted distortion of $S_3$ with central node $v$ and three leaves $v_1, v_2, v_3$.
If we consider three angles at the node $v$, then at least one of them is $\alpha \le 2 \pi / 3$, so we can get a lower bound by only considering this triangle, which is, w.l.o.g., formed by $v, v_1, v_2$.
\[
D_{min} \ge  |d(f(v_1),f(v_2)) - 2| + \frac{|d(f(v_1),f(v)) - 1| + |d(f(v_2),f(v)) - 1|}{(n-1)/2}.
\]

Denote $d(f(v_1),f(v)) = x = 1 + \varepsilon$, $d(f(v_2),f(v)) = y = 1 + \delta$, $d(f(v_1),f(v_2)) = z = 2 + \varepsilon + \delta - \varphi$ with some $\varepsilon, \delta$ and some $\varphi > 0$ (from triangle inequality). Assume that $|\varepsilon| < 1/2$ and $|\delta| < 1/2$ (otherwise the lower bound is trivial). Now we use the law of cosines to get a lower bound on $\varphi$.
We consider Euclidean and hyperbolic spaces separately and note that the bound obtained in Euclidean space also holds in spherical spaces (with any $c$).

In Euclidean space, using triangle inequality, we get  $\varphi = x + y - z > 0$. So, in Euclidean and spherical spaces $\varphi$ is bounded below by a constant.
%\[
%z^2 = x^2 + y^2 - 2 x y \cos \alpha, 
%\]
%so 
%\[
%(x+y)^2 - z^2 = 2 x y \lt{(\cos \alpha+1)}.
%\]
%As $\alpha < \pi$, we get
%$\varphi = x + y - z > 0$. 

In hyperbolic space the law of cosines gives 
(recall that $R = \frac{1}{\sqrt{-c}}$):
\[
\cosh \frac{z}{R} = \cosh \frac{x}{R} \cosh \frac{y}{R} - \sinh \frac{x}{R} \sinh \frac{y}{R} \cos \alpha,
\]
from which, using $\cosh(x+y) =\cosh x\cosh y+\sinh x\sinh y$, we get 
\[
\cosh \frac{x+y}{R} - \cosh \frac{z}{R} = \sinh \frac{x}{R} \sinh \frac{y}{R} (1 + \cos \alpha). %= \Omega\left(e^{(x+y)/R}\right).
\]
If $R \to \infty$, then, similarly to Euclidean case, we get $\varphi = x + y - z = \Omega(1)$. 

%\lt{[Replaced $\phi$ by $\varphi$]}
%\lt{[I do not quite follow the proof below, but the constant $\sqrt{c}$ is better and seems to hold. My idea of the proof was the following: let $x' = x/R, y' = y/R, z' = z/R, \varphi' = \varphi/R$. Then $x',y',z' \to \infty$ when $R \to 0$. Then $\sinh$ and $\cosh$ asymptotically behave as $e^{...}(1+o(1))/2$. Now, if I replace all $\sinh$ in the formula above, I derive that $e^{-\varphi'}$ cannot tend to 1, i.e., $\varphi'$ cannot tend to zero, i.e., $\varphi = \Omega(R)$]}
\iffalse
On the other hand, since $x+y - z = \varphi$, it follows from the mean value theorem that
\[
	\sinh \frac{x}{R} \sinh \frac{y}{R} (1 + \cos \alpha) = \frac{\varphi}{R^2} \sinh \frac{c}{R},
\]
for some $z \le c \le x + y$. Since $\sinh$ is monotonically increasing we deduce that
\[
	\frac{\varphi}{R^2} \ge (1+\cos \alpha) \frac{\sinh \frac{x}{R} \sinh \frac{y}{R}}{\sinh \frac{x+y}{R}}.
\]
Since the right hand side is bounded from below by a constant as $R \to 0$ it follows that $\varphi = \Omega(R^2)$ as $R \to 0$.
%}Otherwise,  have
%\[
%\cosh \frac{x+y}{R} - \cosh \frac{z}{R} = \Omega\left(e^{(x+y)/R}\right) = \Omega\left(e^{1/R}\right)
%\]
%and
%%$e^{\frac{x+y-z}{R}} - 1 = \Omega\left(e^{\frac{x+y-z}{R}}\right) = \Omega(1)$, so 
%$\varphi = x + y - z = \Omega(R)$. \ph{[I still do not follow this last step. Where does this conclusion come from and what are the asymptotics considered? It can't be $R \to \infty$ because we concluded that in that case it is bounded from below by a constant. So is it $R \to 0$?]}
\fi

On the other hand, if $R \to 0$, we get $\varphi = \Omega(R)$.
Note that $D_{min} \ge |z-2| + \frac{|x-1| + |y-1|}{(n-1)/2} =  |\varepsilon + \delta - \varphi| + \frac{|\varepsilon| + |\delta|}{(n-1)/2}$. This gives us a lower bound $D_{min} = \Omega(1/n)$ in spherical and Euclidean spaces and 
$D_{min} = \Omega(\min(R,1)/n) = \Omega\left(\frac{1}{n\max\left(\sqrt{-c},1\right)}\right)$ in hyperbolic space.
From this and \Eqref{eq:D_min} the bound on $D(S_n)$ follows.
%\ph{$D_{min} = \Omega(\min(R^2,1)/n) = \Omega\left(\frac{1}{n\max\left(-c,1\right)}\right)$ in hyperbolic space}. \ph{Shouldn't this be $\min(R^2, 1/n)$?} From this and \Eqref{eq:D_min} the required bound follows.

Now, let us get an upper bound on optimal distortion $D_{opt}(S_n)$. To do this, we explicitly construct an embedding with sufficiently low distortion $D(S_n)$.

Let $v$ be the central node, then we spread all other nodes uniformly on a 2-dimensional circle of radius $1$ centred at $v$. The smallest angle between two points is $2 \pi / n$. Therefore, from the law of cosines, the distance between leaves is at least $k$ with
\[
\cosh \frac{k}{R} = 1  + \left(1 -  \cos \frac{2 \pi}{n}\right)  \sinh^2 \frac{1}{R} \,.
\]

Note that for any two leaves $v_i$ and $v_j$ we have that $d(f(v_i), f(v_j)) \le 2$. In particular, the closer two leaves are, the greater the difference $2 - d(f(v_i), f(v_j))$ is. Hence, the distance between adjacent leaves is the worst case and thus $D_{opt}(S_n)$ can be upper bounded as
\begin{multline*}
D_{opt}(S_n) \le \frac{{n \choose 2}}{2{n+1\choose 2}} \left(2 - R\cdot \mathrm{arccosh}\left( \left(1 - \cos \frac{2 \pi}{n}\right)\sinh^2\frac{1}{R}  + 1 \right)\right) \\
= \frac{(n-1)}{2(n+1)} \left(2 - R\cdot \mathrm{arccosh}\left( \left(1 - \cos \frac{2 \pi}{n}\right)\sinh^2\frac{1}{R}  + 1 \right)\right)
\end{multline*}
Note that $1 - \cos\frac{2\pi}{n} = \Theta\left(\frac{1}{n}\right)$ and $\sinh^2\left(\frac{1}{R}\right) = \Theta\left(e^{2/R}\right)$.
Then, $\textrm{arccosh}\left(\Theta\left(\frac{1}{n} e^{2/R}\right) + 1 \right)$ behaves as $\sqrt{2e^{2/R}/n}$ if $2/R \ll \log n$ and as $\frac{2}{R} - \log n$ if $2/R \gg \log n$.
Therefore, we get
\[
D_{opt}(S_n) = O\left( R \log n \right) = O\left( \frac{\log n}{\sqrt{-c}} \right).
\]

\subsection{Proof of Theorem~\ref{thm:clique_distortion} (distortion-based curvature for complete graphs)}\label{app:clique_distortion}

If $d = n-2$, then we are given a $(n-1)$-simplex, which can be embedded into $n-2$-dimensional spherical space. Indeed, the radius of circumscribed hypersphere for the $(n-1)$-simplex with side length $a$ is known to be $R = a\sqrt{\frac{n-1}{2n}}$. Since we want the \textit{spherical} distance between all points to be equal to one, we need to choose $a$ accordingly:
\[
\sin \frac{\alpha}{2} = \frac{a}{2R} \,\,\text{ for } \,\,\alpha = \frac{1}{R}.
\]
Here $\alpha$ corresponds to the angle giving the arc length 1, while the condition on $\sin \frac{\alpha}{2}$ relates $\alpha$ and $a$ since $\alpha$ is the angle in a triangle with side lengths $\alpha, R, R$.
This implies that
\[
2 R \arcsin \frac{a}{2R} = 1,
\]
and solving this equation for $a$ yields
%\[
%2 a\sqrt{\frac{n-1}{2n}} \arcsin \sqrt{\frac{n}{2(n-1)}} = 1,
%\]
\[
a  = \sqrt{\frac{2n}{(n-1)}}\frac{1}{2\arcsin \sqrt{\frac{n}{2(n-1)}}}.
\]
Plugging this back into the formula for the radius $R$ we obtain
\[
R = \frac{1}{2\arcsin \sqrt{\frac{n}{2(n-1)}}},
\]
so that we have
\[
C_{n-2}^{dist}(K_n) = 4 \left(\arcsin \sqrt{\frac{n}{2(n-1)}}\right)^2\,.
\]

Finally, let us show that if $c \to -\infty$, then optimal distortion $D_{opt}(K_n) \to 0$. This result follows from the fact that $D_{opt}(S_n) \to 0$, because to embed a clique, it is sufficient to embed a star on $n+1$ node with edge lengths 1/2 and then remove the central node. 

\subsection{Proof of Theorem~\ref{thm:star_threshold} (threshold-based curvature for star)}\label{app:star_threshold}

We show below that for any $n$, any dimension $d$ and some curvature $c$ there exists a perfect embedding.
Therefore $C_d^{dist}(S_n)$ consists of curvatures for which such perfect embedding exists. 

First, let us note that if there exists a perfect embedding $f$ for some curvature $c$, then there exists a perfect embedding for any curvature $c' < c$. Indeed, w.l.o.g., we assume that the central node $v$ is mapped to the origin of a hyperspherical coordinate system and other points $v_1, \ldots, v_n$ can be described by their radii and angles. We know that the distance between $v$ and any $v_i$ is at most 1 and the distance between any pair $v_i, v_j$ is larger than one. Now we change curvature to $c' < c$ and keep hyperspherical coordinates the same. Then the distance between $v$ and any $v_i$ does not change, while the distance between nodes $v_i, v_j$ increases.

Now, it is easy to see that $C$ increases with $d$: if there exists an embedding to some dimension $d$, then, obviously, the same embedding works for $d' > d$. Further, $C$ decreases with $n$ since if there exists embedding of $S_n$, then we can easily construct an embedding of $S_{n'}$ for $n' < n$ by removing some nodes. 

Now, let us construct a perfect embedding of $S_n$
and estimate the required curvature.
Recall that in a perfect embedding all leaves have to be inside the ball of radius 1 around the central node $v$ and also the distance between any two leaves has to be larger than one. 

It is easy to see that if we managed to spread $n$ points inside the ball of radius 1 with distances more than 1 between them, then we can move each point along the radius up to distance 1 from $v$ preserving this property. Therefore, it is sufficient to spread all points on a hypersphere. 

Let us first consider $d = 2$. In this case we can find an explicit analytic expression for $C$. This will be an upper bound for any $d>2$.

First, assume that $n > 6$. In this case we have to consider only hyperbolic space, since $n$ neighbors would not fit to a circle of radius 1 in neither spherical or Euclidean spaces.

We will find such largest curvature $C$ which allows to have distance exactly 1 between the closest leaves. In this case we cannot embed $S_n$ in a space of curvature $C$, but can embed in a space of any smaller curvature. 
We use \Eqref{eq:hyp_eq} and let $\alpha = \frac{2\pi}{n}$:
%\[
%\cosh{\frac{1}{k}} = \cosh^2{\frac{1}{k}} - \sinh^2{\frac{1}{k}}\cos\alpha.
%\]
%\[
%\cosh{\frac{1}{k}} = 1 +  (1- \cos\alpha) \sinh^2{\frac{1}{k}}.
%\]
%\[
%\cosh{\frac{1}{k}} = 1 +  2\sin^2 \frac{\alpha}{2} \sinh^2{\frac{1}{k}}.
%\]
%\[
%\frac{1}{2\sin^2 \frac{\alpha}{2}} = \frac{\sinh^2{\frac{1}{k}}}{\cosh{\frac{1}{k}} - 1}.
%\]
%\[
%\frac{1}{2\sin \frac{\alpha}{2}} = \frac{\sinh{\frac{1}{k}}}{\sqrt{2}\sqrt{\cosh{\frac{1}{k}} - 1}}.
%\]
\[
\cosh\frac{1}{2R} = \frac{1}{2\sin \frac{\alpha}{2}},
\]
\[
R = \frac{1}{2\,\textrm{arccosh}\frac{1}{2\sin \frac{\alpha}{2}}},
\]
\[
C = - \left(2\,\textrm{arccosh}\frac{1}{2\sin \frac{\alpha}{2}}\right)^2.
\]

Note that if $n$ is large, then $\sin \frac{\alpha}{2} = \sin\frac{\pi}{n-1} \sim \frac{\pi}{n-1}$. Then,  $\textrm{arccosh}\frac{1}{2\sin \frac{\alpha}{2}} \sim \textrm{arccosh}\frac{n-1}{2\pi} \sim \log n$, so we get $C \sim - 4 \log^2 n$. 

Now, let us consider $n \le 6$. Obviously, for $n = 6$ we have $C = 0$.

If $n < 6$, then $C > 0$. We use \Eqref{eq:sph_eq}:
\[
\cos{\frac{1}{R}} = \frac{\cos \alpha}{1 - \cos \alpha},
\]
\[
C = \left(\arccos \frac{\cos \alpha}{1 - \cos \alpha}\right)^2.
\]

%\textcolor{red}{[For $d> 2$ we can get lower and upper bounds using spherical code / kissing number, but I don't have the proofs yet.]} 

%About upper bound. If we have $n$ points at distance at least 1 from each other, then the spheres of radius 1/2 around them do no intersect. Therefore, we can find the volume that we need to get such spheres. 

%For $d > 2$ we can get lower and upper bounds for $C$. One bound we can obtain by covering the surface of a unit ball by the balls of radius $1/2$. The other bound can be obtained by using spherical packing. \textbf{[To be finished formally.]} 

\subsection{Proof of Theorem~\ref{thm:tree_threshold} (threshold-based curvature for trees)}\label{app:tree_threshold}

%\textcolor{red}{[It will be good to also get upper bound here.]}

%\textcolor{red}{[The proof below I wrote but did not re-read, so it can be not very detailed.]}

As for $S_n$, we prove that for $T_b$ a perfect embedding exists for $d = 2$ and some curvature $c$. Then, similarly to the previous section, it is clear that $C$ increases with $b$ and decreases with $n$.

%First, let us discuss the upper bound. 
%Here we say that all children at levels $l, l-2, l-4, \ldots$ have to fit to the ball of radius $l$ with all pairwise distances $> 1$. We can consider large enough $l$ here (and get hyperbolic space).

\begin{figure}
    \centering
    \includegraphics[width = 0.5 \textwidth]{trees.pdf}
    \caption{Threshold-based embedding of trees}
    \label{fig:trees}
\end{figure}


For the lower bound on $C$, we have to guarantee that an embedding exists. For this, we provide the following construction (see Figure~\ref{fig:trees} for an illustration).
(Below we assume that the curvature is large enough for our construction to work, then we estimate the required curvature.)
First, we take the node $v$ and consider a circle of radius 1 around this node. 
We spread $b + 1$ neighbors uniformly around this node. For our construction to work, we need all distances between these nodes to be larger than 1.
Now, at some step of the algorithm, assume that we have all nodes at level $l$ placed at some circle centered at $v$ and all distances between the nodes at level $l$ are larger than 1. Our aim is then to find positions for all nodes at level $l + 1$.

Let us take any node at $l$-th level. Consider two points $u_l$ and $u_r$ on the same circle at distance 1 from the node $u$ to the left and to the right, respectively.
Let $u,u_l,z_l$ and $u,u_r,z_r$ form equilateral triangles (with sides equal to 1). Then we let the points at $l+1$-th level to be spread on the circle centred at $v$ and passing through $z_l$ and $z_r$. The children of $u$ ($u_1, \ldots, u_{b}$) will be placed on the circular arc between $z_l$ and $z_r$. As usual, we want $u_i$ and $u_{i+1}$ to be at distance at least 1 from each other. Moreover, they have to be at distance at least 1 from children of other nodes. Also, note that placing $u_1, \ldots, u_{b}$ between $z_l$ and $z_r$ guarantees that these nodes are closer than 1 to their parent node $u$ but at the same time at a distance larger than 1 from all other nodes at $l$-th level. Also, all points at $l$-th level are far enough from points at $l+2$-th level. 

It remains to find a maximum curvature such that the required conditions are satisfied. Let $r$ and $r'$ be radii of circles at $l$-th and $l+1$-th levels and let $2\alpha = \angle u_l v u$. We know (the law of cosines and $\cos 2\alpha = 1 - 2\sin^2 \alpha$) that
\begin{equation}\label{eq:1}
\cosh \frac{1}{R} = 1 + 2 \sinh \frac{r}{R} \sin^2 \alpha \,.
\end{equation}
And the only condition we need for the whole procedure to work is that we have enough space on the circular arc for placing $b$ nodes there:
\[
\cosh \frac{1}{R} \le 1 + 2 \sinh \frac{r'}{R} \sin^2 \frac{\alpha}{b} \,.
\]
Now we note that $\sin^2 \frac{\alpha}{b} \ge \frac{\sin^2 \alpha}{b^2}$ for all $b \ge 1$. Therefore, it is sufficient to have
\begin{equation}\label{eq:2}
\cosh \frac{1}{R} \le 1 + 2 \sinh \frac{r'}{R} \cdot \frac{\sin^2 \alpha}{b^2} \,.
\end{equation}
Combining \Eqref{eq:1} and \Eqref{eq:2}, we obtain:
\[
\sinh \frac{r'}{R} \ge b^2 \cdot \sinh \frac{r}{R}.
\]
To achieve this, it is sufficient to have: 
\[
\frac{r' - r}{R} \ge 2 \log b,
\]
\[
R \le \frac{r' - r}{2 \log b}.
\]
It remains to find the lower bound for $r' - r$ and it is easy to see that $r' - r$ decreases with $r$. Therefore, it is sufficient to consider only the second step of the construction procedure, when we move from the circle of radius 1 to the next one. In this case, $r = 1$ and 
\[
r' = 2 \, \mathrm{arccosh} \frac{\cosh 1}{\cosh 1/2}.
\]
So, we have 
\[
-C = \frac{1}{R^2} \le \left( \frac{2\,\log b}{2 \, \mathrm{arccosh} \frac{\cosh 1}{\cosh 1/2} - 1} \right)^2.
\]

\section{Combinations of simple graphs}\label{sec:combinations}

In this section, in order to get additional intuition, we illustrate how combining several simple subgraphs may affect the optimal curvature. 
We choose distortion for this illustrative example, since for this loss we theoretically obtained non-trivial results: for some simple graphs the optimal curvature is negative, while for other it is positive.

Indeed, for trees (and, in particular, stars) the optimal curvature is minus infinity and for cycles the optimal curvature is positive. Our intuition is that combining stars and cycles in one graph would give optimal curvature located between the curvatures of these simple graph. 

Indeed, we observe such expected behavior for two illustrative examples shown in Figures~\ref{fig:star} and~\ref{fig:stars}. In Figure~\ref{fig:star} we take a cycle of length 10 and iteratively add edges to one of its nodes, i.e., we create a star on one node. In Figure~\ref{fig:stars} we take the same cycle and add small stars sequentially to several of its nodes: in the left figure we just add one dangling edge for each node (i.e., create a star subgraph on 4 nodes), in the right figure  we add two edges. In all cases, the optimal curvature drifts from a positive one for a cycle to negative values. This drift is more pronounced In Figure~\ref{fig:star}, where we create just one (relatively) large star.

\begin{figure}
    \centering
    \includegraphics[width = 0.49 \textwidth]{illustration_star.pdf}
    \caption{Adding $k$ edges to one node of a cycle on $10$ nodes}\label{fig:star}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width = 0.49 \textwidth]{illustration_stars_1.pdf}
    \includegraphics[width = 0.49 \textwidth]{illustration_stars_2.pdf}
    \caption{Adding one (left) or two (right) dangling edges to $k$ subsequent nodes of a cycle on $10$ nodes}\label{fig:stars}
\end{figure}



\section{Volume-based curvature approximation}\label{sec:new_curvature}

In this section, we describe the proposed curvature approximation. An input parameter of this estimator is the maximum neighborhood size $k_{max}$ (we fix $k_{max} = 3$ in the experiments). We take a node $v$ and for any $k \le k_{max}$ we compute the number of nodes $N_k(v)$ at distance exactly $k$ from it.
Then, we discount this value using the number of edges between the nodes.
This captures our intuition that we need enough space to place the neighbors, but the connected neighbors are not required to be far away from each other.
Formally, let $E_k(v)$ be the number of non-edges between the pairs of nodes at distance $k$. Then, we say that $N_k'(v)$ is such that ${N_k'(v) \choose 2} = E_k(v)$.  Finally, we compute the curvature $c_k(v)$ such that the area of a hypersphere or radius $k$ is equal to $N_k'(v)$. And our estimate for $v$ is $c(v) = \min_{k \le k_{max}} c_k(v)$ (the worst-case curvature). To get a curvature of the whole graph, we take a median of all $c_k(v)$. (Similarly to other curvature estimators, one can consider a subsample of nodes instead of the whole node set to speed up the computation).

\section{Experiments}

\subsection{Datasets}\label{sec:datasets}

Datasets used in our experiments are listed in Table~\ref{tab:datasets}. In all cases, we take only the giant connected component (numbers of nodes and edges in Table~\ref{tab:datasets} correspond to already filtered dataset). Synthetic datasets are described in Section~\ref{sec:theory}. The dataset $T_{3,6}$ is a tree with branching factor 3 and depth 6.

\begin{table}[t]
\caption{Dataset description}
\label{tab:datasets}
\begin{center}
\begin{tabular}{lcc}
%\multicolumn{1}{c}{\bf PART}  &\multicolumn{1}{c}{\bf DESCRIPTION}
Dataset & num. nodes & num. edges \\
 \hline \\
%Karate club~\citep{zachary1977information} 
%    	& 34 & 78   \\
Conflict~\citep{ward2007disputes} & 127 & 253 \\
Chicago~\citep{eash1979equilibrium} & 822 & 821 \\
CSPhDs~\citep{nooy2006exploratory} & 1025 & 1043 \\
Euroroad~\citep{vsubelj2011robust} &  1039 & 1305 \\
EuroSiS~\citep{khokhar2015gephi} & 1272 & 6424 \\
Power~\citep{watts1998collective} & 4941 & 6594 \\
Facebook~\cite{leskovec2012learning} & 4039 & 88234 \\
\end{tabular}
\end{center}
\end{table}

\subsection{Experimental setup}\label{sec:setup}

Our experiments are based on a publicly available implementation of graph embedding by~\citet{gu2019learning}.\footnote{\url{https://github.com/HazyResearch/hyperbolics}} The main advantage of this algorithm is that it works for any curvature: negative, positive or zero. 

We modified the implementation for our task: in particular, we added a regime responsible for minimizing zero-one loss function. 
%Let $e_{ij}$ be the adjacency matrix of a graoh $G$, 
Zero-one loss is equal to
\[
\sum_{i,j}  I[v_i \sim v_j] \cdot I[d(f(v_i)f(v_j) > 1] + (1 - I[v_i \sim v_j]) \cdot I[d(f(v_i),f(v_j)) \le 1].
\]
This function is not differentiable, so we replace it by gradient descent friendly function which we call ReLuLoss:
\[
\sum_{i,j}  I[v_i \sim v_j] \cdot \mathrm{ReLu}(d(f(v_i),f(v_j)) - (1 - \varepsilon)) + (1 - I[v_i \sim v_j]) \cdot \mathrm{ReLu}((1 + \varepsilon) -d(f(v_i),f(v_j)),
\]
where 
$ \mathrm{ReLu}(x) = \max(0, x), \varepsilon = 0.001$.
In our experiments we observed that such loss function outperforms other analogues (e.g., MSE and sigmoid-based smoothing).

Note that in experiments we use only the more advanced Forman curvature $\hat{F}(G)$.
For computing Ollivier and Forman curvature we use the open source package by~\citet{ni2015ricci}.\footnote{\url{https://github.com/saibalmars/GraphRicciCurvature}}

For the embedding algorithm, in all experiments we fix \textit{epochs = 1000} and we also try several learning rates (usually from 0.01 to $10^4$). We noticed that learning rate significantly affect the performance of the learning algorithm and also the optimal learning rate is usually smaller for smaller datasets and it is larger for spaces of negative curvature. We also use the option \textit{use\_adagrad = True} since we noticed that it inproves the embedding into Euclidean spaces (otherwise there is a peak in distortion at $c = 0$).

Let us emphasize that in our experiments we do not care about overfitting, because we do not analyze the embedding algorithm, our aim is to find a curvature which minimizes loss function. (It can be also thought of as though we focus on graph reconstruction task).

To plot all the figures we try curvatures in some range (usually between -2 and 1) with step size 0.1.

Our code is publicly available.\footnote{\url{https://drive.google.com/open?id=12i5LD6yTyFRDrgkMJEencDAI0Au6Isrn}}

\subsection{Comparison of curvatures on synthetic datasets}\label{sec:compare_synth}

See the results in Tables~\ref{tab:compare_distortion_synth} and~\ref{tab:compare_zero_one_synth}.

\begin{table}[t]
\caption{Compare curvature estimators, distortion, synthetic datasets}
\label{tab:compare_distortion_synth}
\begin{center}
\begin{tabular}{lcccccc|cc}
&
\multicolumn{2}{c}{Ollivier} &
\multicolumn{2}{c}{Forman} &
\multicolumn{2}{c}{Avg. sectional} &
\multicolumn{2}{|c}{Grad. descent} \\
%\multicolumn{2}{c}{Grid search} \\
Dataset/dim & c & loss  & c & loss  & c & loss  & c & loss \\% & c & loss \\
 \hline \\
$S_{100}$ / 2 &
0.0 & 0.334 & 
-97.0 & \textbf{0.058} & 
-1.0 & 0.304 &
-1.45 & 0.289 \\
%-2.0 & 0.275 \\
$S_{100}$ / 10 &
0.0 & 0.127 & 
-97.0 & \textbf{0.014} & 
-1.0 & 0.1 & 
-30.2 & 0.025 \\
%-2.0 & 0.083\\
$T_{3,6}$ / 2 &
-0.33 & \textbf{0.136} & 
-2.0 & 0.254 & 
-0.5 & 0.174 & 
-9.77 & 0.271  \\
%-0.2 & 0.141 \\
$T_{3,6}$ / 10 &
-0.33 & \textbf{0.016} & 
-2.0 & 0.243 & 
-0.5 & 0.036 & 
0.0 & 0.077 \\
%-0.2 & 0.017 \\
$K_{100}$ / 2 & 
0.99 & 0.348 & 
100 & 0.841 & 
0.125 & \textbf{0.342} & 
-48.7 & \textbf{0.163} \\
%3.6 & 0.322 \\
$K_{100}$ / 10 & 
0.99 & 0.134 & 
100 & 0.841 & 
0.125 & \textbf{0.124} & 
-305 & \textbf{0.015} \\
%-1.9 & 0.108 \\
$C_{100}$ / 2 & 
0.0 & \textbf{0.106} & 
0.0 & \textbf{0.106} & 
0.01 & 0.257 &
0.0 & \textbf{0.106}  \\
%0.0 & 0.106 \\
$C_{100}$ / 10 & 
0.0 & \textbf{0.106} & 
0.0 & \textbf{0.106} & 
0.01 & 0.255 & 
0.0 & \textbf{0.106} \\
%0.0 & 0.106 \\
$K_{100,100}$ / 2 &
0.0 & 0.372 & 
-196 & \textbf{0.294} & 
0.008 & 0.372 & 
2.1 & 0.309 \\
%2.2 & 0.306 \\
$K_{100,100}$ / 10 &
0.0 & 0.301 & 
-196 & \textbf{0.275} & 
0.008 & 0.301 & 
-139 & \textbf{0.27} \\
%2.4 & 0.250 \\
\end{tabular}
\end{center}
\end{table}

\begin{table}[t]
\caption{Compare curvature estimators, zero-one loss, synthetic datasets}
\label{tab:compare_zero_one_synth}
\begin{center}
\begin{tabular}{lcccccccc|cc}
&
\multicolumn{2}{c}{Ollivier} &
\multicolumn{2}{c}{Forman} &
\multicolumn{2}{c}{Avg. sectional} &
\multicolumn{2}{c}{Volume} &
\multicolumn{2}{|c}{Grad. descent} 
%& \multicolumn{2}{c}{Grid search} 
\\
Dataset & c & loss  & c & loss  & c & loss & c & loss & c & loss  
\\
\hline \\
$S_{100}$ / 2 &
0.0 & 0.02 & 
-97.0 & \textbf{0.004} & 
-1.0 & 0.027 &
-4.39 & 0.033 &
0.0 & 0.02
\\
$T_{3,6}$ / 2 &
-0.33 & 0.006 & 
-2.0 & 0.005 & 
-0.5 & 0.005 & 
0.0 & \textbf{0.002}  &
0.0 & \textbf{0.002}  
%& 0.0 & 0.002 
\\
$C_{100}$ / 2 & 
0.0 & \textbf{0.005} & 
0.0 & \textbf{0.005} &
0.01 & 0.009 & 
0.0 & \textbf{0.005} &
0.16 & 0.012
%& 0.0 & 0.004 
\\
$K_{100,100}$ / 2 &
0.0 & 0.496 & 
-196 & \textbf{0.492} & 
0.008 & 0.496 & 
-25.9 & 0.494 & 
-531 & 0.493 
%& 0.5 & 0.486 
\\
$K_{100,100}$ / 10 &
0.0 & 0.489 & 
-196 & 0.491 & 
0.008 & \textbf{0.480} & 
-0.93 & \textbf{0.480} &
-124 & 0.490 
\\
\end{tabular}
\end{center}
\end{table}

\subsection{Effect of dimension and loss function}\label{sec:figures}

In this section, we visualize the dependence of the loss function on curvature (see Figures~\ref{fig:karate}-\ref{fig:power}). In particular, we demonstrate that the optimal curvature significantly depends on both loss function and dimension. The figures support our intuition that while hyperbolic space helps in many cases for small dimensions, it becomes less pronounced in larger dimensions. 

\begin{figure}
    \centering
    \includegraphics[width = 0.49 \textwidth]{karate_distortion.pdf}
    \includegraphics[width = 0.49 \textwidth]{karate_zero_one.pdf}
    \includegraphics[width = 0.49 \textwidth]{karate_pearson.pdf}
    \includegraphics[width = 0.49 \textwidth]{karate_balanced_zero_one.pdf}
    \caption{Effect of curvature, Karate club dataset}
    \label{fig:karate}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width = 0.49 \textwidth]{conflict_distortion.pdf}
    \includegraphics[width = 0.49 \textwidth]{conflict_zero_one.pdf}
    \includegraphics[width = 0.49 \textwidth]{conflict_pearson.pdf}
    \includegraphics[width = 0.49 \textwidth]{conflict_balanced_zero_one.pdf}
    \caption{Effect of curvature, Conflict dataset}
    \label{fig:conflict}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width = 0.49 \textwidth]{dolphins_distortion.pdf}
    \includegraphics[width = 0.49 \textwidth]{dolphins_zero_one.pdf}
    \includegraphics[width = 0.49 \textwidth]{dolphins_pearson.pdf}
    \includegraphics[width = 0.49 \textwidth]{dolphins_balanced_zero_one.pdf}
    \caption{Effect of curvature, Dolphins dataset}
    \label{fig:conflict}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width = 0.49 \textwidth]{chicago_distortion.pdf}
    \includegraphics[width = 0.49 \textwidth]{chicago_zero_one.pdf}
    \caption{Effect of curvature, Chicago dataset}
    \label{fig:chicago}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width = 0.49 \textwidth]{CSPhDs_distortion.pdf}
    \includegraphics[width = 0.49 \textwidth]{CSPhDs_zero_one.pdf}
    \caption{Effect of curvature, CSPhDs dataset}
    \label{fig:CSPhDs}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width = 0.49 \textwidth]{euroroad_distortion.pdf}
    \includegraphics[width = 0.49 \textwidth]{euroroad_zero_one.pdf}
    \caption{Effect of curvature, Euroroad dataset}
    \label{fig:euroroad}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width = 0.49 \textwidth]{EuroSiS_distortion.pdf}
    \includegraphics[width = 0.49 \textwidth]{EuroSiS_zero_one.pdf}
    \caption{Effect of curvature, EurodSiS dataset}
    \label{fig:EurodSiS}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width = 0.49 \textwidth]{power_distortion.pdf}
    \includegraphics[width = 0.49 \textwidth]{power_zero_one.pdf}
    \caption{Effect of curvature, Power dataset}
    \label{fig:power}
\end{figure}


\end{document}





\section{Default Notation}

In an attempt to encourage standardized notation, we have included the
notation file from the textbook, \textit{Deep Learning}
\cite{goodfellow2016deep} available at
\url{https://github.com/goodfeli/dlbook_notation/}.  Use of this style
is not required and can be disabled by commenting out
\texttt{math\_commands.tex}.


\centerline{\bf Numbers and Arrays}
\bgroup
\def\arraystretch{1.5}
\begin{tabular}{p{1in}p{3.25in}}
$\displaystyle a$ & A scalar (integer or real)\\
$\displaystyle \va$ & A vector\\
$\displaystyle \mA$ & A matrix\\
$\displaystyle \tA$ & A tensor\\
$\displaystyle \mI_n$ & Identity matrix with $n$ rows and $n$ columns\\
$\displaystyle \mI$ & Identity matrix with dimensionality implied by context\\
$\displaystyle \ve^{(i)}$ & Standard basis vector $[0,\dots,0,1,0,\dots,0]$ with a 1 at position $i$\\
$\displaystyle \text{diag}(\va)$ & A square, diagonal matrix with diagonal entries given by $\va$\\
$\displaystyle \ra$ & A scalar random variable\\
$\displaystyle \rva$ & A vector-valued random variable\\
$\displaystyle \rmA$ & A matrix-valued random variable\\
\end{tabular}
\egroup
\vspace{0.25cm}

\centerline{\bf Sets and Graphs}
\bgroup
\def\arraystretch{1.5}

\begin{tabular}{p{1.25in}p{3.25in}}
$\displaystyle \sA$ & A set\\
$\displaystyle \R$ & The set of real numbers \\
$\displaystyle \{0, 1\}$ & The set containing 0 and 1 \\
$\displaystyle \{0, 1, \dots, n \}$ & The set of all integers between $0$ and $n$\\
$\displaystyle [a, b]$ & The real interval including $a$ and $b$\\
$\displaystyle (a, b]$ & The real interval excluding $a$ but including $b$\\
$\displaystyle \sA \backslash \sB$ & Set subtraction, i.e., the set containing the elements of $\sA$ that are not in $\sB$\\
$\displaystyle \gG$ & A graph\\
$\displaystyle \parents_\gG(\ervx_i)$ & The parents of $\ervx_i$ in $\gG$
\end{tabular}
\vspace{0.25cm}


\centerline{\bf Indexing}
\bgroup
\def\arraystretch{1.5}

\begin{tabular}{p{1.25in}p{3.25in}}
$\displaystyle \eva_i$ & Element $i$ of vector $\va$, with indexing starting at 1 \\
$\displaystyle \eva_{-i}$ & All elements of vector $\va$ except for element $i$ \\
$\displaystyle \emA_{i,j}$ & Element $i, j$ of matrix $\mA$ \\
$\displaystyle \mA_{i, :}$ & Row $i$ of matrix $\mA$ \\
$\displaystyle \mA_{:, i}$ & Column $i$ of matrix $\mA$ \\
$\displaystyle \etA_{i, j, k}$ & Element $(i, j, k)$ of a 3-D tensor $\tA$\\
$\displaystyle \tA_{:, :, i}$ & 2-D slice of a 3-D tensor\\
$\displaystyle \erva_i$ & Element $i$ of the random vector $\rva$ \\
\end{tabular}
\egroup
\vspace{0.25cm}


\centerline{\bf Calculus}
\bgroup
\def\arraystretch{1.5}
\begin{tabular}{p{1.25in}p{3.25in}}
% NOTE: the [2ex] on the next line adds extra height to that row of the table.
% Without that command, the fraction on the first line is too tall and collides
% with the fraction on the second line.
$\displaystyle\frac{d y} {d x}$ & Derivative of $y$ with respect to $x$\\ [2ex]
$\displaystyle \frac{\partial y} {\partial x} $ & Partial derivative of $y$ with respect to $x$ \\
$\displaystyle \nabla_\vx y $ & Gradient of $y$ with respect to $\vx$ \\
$\displaystyle \nabla_\mX y $ & Matrix derivatives of $y$ with respect to $\mX$ \\
$\displaystyle \nabla_\tX y $ & Tensor containing derivatives of $y$ with respect to $\tX$ \\
$\displaystyle \frac{\partial f}{\partial \vx} $ & Jacobian matrix $\mJ \in \R^{m\times n}$ of $f: \R^n \rightarrow \R^m$\\
$\displaystyle \nabla_\vx^2 f(\vx)\text{ or }\mH( f)(\vx)$ & The Hessian matrix of $f$ at input point $\vx$\\
$\displaystyle \int f(\vx) d\vx $ & Definite integral over the entire domain of $\vx$ \\
$\displaystyle \int_\sS f(\vx) d\vx$ & Definite integral with respect to $\vx$ over the set $\sS$ \\
\end{tabular}
\egroup
\vspace{0.25cm}

\centerline{\bf Probability and Information Theory}
\bgroup
\def\arraystretch{1.5}
\begin{tabular}{p{1.25in}p{3.25in}}
$\displaystyle P(\ra)$ & A probability distribution over a discrete variable\\
$\displaystyle p(\ra)$ & A probability distribution over a continuous variable, or over
a variable whose type has not been specified\\
$\displaystyle \ra \sim P$ & Random variable $\ra$ has distribution $P$\\% so thing on left of \sim should always be a random variable, with name beginning with \r
$\displaystyle  \E_{\rx\sim P} [ f(x) ]\text{ or } \E f(x)$ & Expectation of $f(x)$ with respect to $P(\rx)$ \\
$\displaystyle \Var(f(x)) $ &  Variance of $f(x)$ under $P(\rx)$ \\
$\displaystyle \Cov(f(x),g(x)) $ & Covariance of $f(x)$ and $g(x)$ under $P(\rx)$\\
$\displaystyle H(\rx) $ & Shannon entropy of the random variable $\rx$\\
$\displaystyle \KL ( P \Vert Q ) $ & Kullback-Leibler divergence of P and Q \\
$\displaystyle \mathcal{N} ( \vx ; \vmu , \mSigma)$ & Gaussian distribution %
over $\vx$ with mean $\vmu$ and covariance $\mSigma$ \\
\end{tabular}
\egroup
\vspace{0.25cm}

\centerline{\bf Functions}
\bgroup
\def\arraystretch{1.5}
\begin{tabular}{p{1.25in}p{3.25in}}
$\displaystyle f: \sA \rightarrow \sB$ & The function $f$ with domain $\sA$ and range $\sB$\\
$\displaystyle f \circ g $ & Composition of the functions $f$ and $g$ \\
  $\displaystyle f(\vx ; \vtheta) $ & A function of $\vx$ parametrized by $\vtheta$.
  (Sometimes we write $f(\vx)$ and omit the argument $\vtheta$ to lighten notation) \\
$\displaystyle \log x$ & Natural logarithm of $x$ \\
$\displaystyle \sigma(x)$ & Logistic sigmoid, $\displaystyle \frac{1} {1 + \exp(-x)}$ \\
$\displaystyle \zeta(x)$ & Softplus, $\log(1 + \exp(x))$ \\
$\displaystyle || \vx ||_p $ & $\normlp$ norm of $\vx$ \\
$\displaystyle || \vx || $ & $\normltwo$ norm of $\vx$ \\
$\displaystyle x^+$ & Positive part of $x$, i.e., $\max(0,x)$\\
$\displaystyle \1_\mathrm{condition}$ & is 1 if the condition is true, 0 otherwise\\
\end{tabular}
\egroup
\vspace{0.25cm}


\paragraph{Circle}

Consider a circle on $n$ nodes and let $v \sim u$ be two neighbors. Then $W_1^G(m_u,m_v) = 1$ and hence $\kappa_G(u,v) = 0$.



\paragraph{Lattice}
Consider the $2$-dimensional lattice $\mathbb{Z}^2$ and let $v \sim u$ be two neighbors. Then the optimal transportation from $m_u$ to $m_v$ is given by a translation by the vector $y - x$ and hence $W_1^G(m_v,m_u) = d_G(u,v) = 1$, see also \cite[Example 5]{ollivier2009ricci}, and thus $\kappa_G(u,v) = 0$.

\paragraph{Complete graph}

Consider a complete graph on $n$ nodes. Then, for any two nodes $u$ and $v$ it follows from Example 1 in~\cite{jost2014ollivier} that $\kappa_G(m_v, m_u) = \frac{n-2}{n-1} \to 1$ as $n \to \infty$.

\paragraph{Erd\H{o}s-Renyi graph}

The following results from~\cite{lin2011ricci} characterize Ollivier-Ricci curvature in $G(n,p)$:

\begin{enumerate}
\item If $p \ge \log(n)^{1/3} n^{-1/3}$ then almost surely
\[
	\kappa_G(u,v) = p + O\left(\sqrt{\frac{\log(n)}{n p}}\right).
\]
\item If $2 \log(n)^{1/2} n^{-1/2} \le p < \log(n)^{1/3} n^{-1/3}$ then almost surely
\[
	\kappa_G(u,v) = O\left(\frac{\log(n)}{n p^2}\right).
\]
\item If $\log(n)^{1/2} n^{-2/3} \ll p \ll \log(n)^{-1/2}$ then almost surely
\[
	\kappa_G(u,v) = -1 + O\left(n p^2\right) + O\left(\frac{\log(n)}{n^2 p^3}\right).
\]
\item If $\log(n) n^{-1} \ll p \ll n^{-2/3}$ then almost surely
\[
	\kappa_G(u,v) = -2 + O\left(n^2 p^3\right) + O\left(\sqrt{\frac{\log(n)}{np}}\right)
\]
\end{enumerate}